{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodianak/matsim-example-project/blob/master/Bachelor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjYRWKcvZ4OK"
      },
      "source": [
        "The following algorithms will be applied onto datasets, to analyze the employment of dependencies in data anonymization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzV0XApEbUXS"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-Cmi-ZObSlv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from os import uname\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "import math\n",
        "import numpy as np\n",
        "import itertools\n",
        "from scipy.stats import entropy\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqMGKWJ6vQbj"
      },
      "source": [
        "###Loading of datasets and data cleansing\n",
        "\n",
        "One of the datasets used for this work is the Adult dataset:\n",
        "```\n",
        "\n",
        "  author       = {Becker, Barry and Kohavi, Ronny},\n",
        "  title        = {{Adult}},\n",
        "  year         = {1996},\n",
        "  howpublished = {UCI Machine Learning Repository},\n",
        "  note         = {{DOI}: https://doi.org/10.24432/C5XW20}\n",
        "\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSyMjSuyK9nN"
      },
      "source": [
        "After removing all the rows with missing values, the size of the dataset went from 48842 entries to 45221."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3KbiqSgvpkD",
        "outputId": "04f70f27-9a2a-4577-83db-2bcec11d4f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "173.0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "path = \"/content/drive/MyDrive/Datasets/adult.csv\"\n",
        "\n",
        "# inserting the names of the columns, for better readability\n",
        "name_columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\",\n",
        "                \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\",\n",
        "                \"native-country\", \"income\"]\n",
        "\n",
        "#adult = pd.read_csv(path, delimiter=\";\", header=None, names = name_columns)\n",
        "\"\"\" get information from the dataset\"\"\"\n",
        "#adult.info()\n",
        "\n",
        "\"\"\" delete records with missing values\"\"\"\n",
        "#adult.replace(' ?', np.nan, inplace=True)\n",
        "#adult.dropna(inplace=True)\n",
        "\n",
        "\"\"\"save cleaned dataset\"\"\"\n",
        "\n",
        "#adult.to_csv(\"/content/drive/MyDrive/Datasets/adult_clean.csv\", index=False)\n",
        "\n",
        "adult = pd.read_csv(\"/content/drive/MyDrive/Datasets/adult_clean.csv\")\n",
        "#adult.info()\n",
        "print(adult[\"hours-per-week\"].unique().max())\n",
        "\n",
        "# obesity dataset\n",
        "obesityDataset = pd.read_csv(\"/content/drive/MyDrive/Datasets/ObesityDataSet.csv\")\n",
        "#obesityDataset.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fen4cxat7hkj"
      },
      "source": [
        "#Computing L-Diversity\n",
        "\n",
        "The algorithm \"l_mondrian\" is based on the multidimensional Mondrian algorithm for k-anonymity by LeFevre et. al.\n",
        "\n",
        "\n",
        "```\n",
        "  author={LeFevre, K. and DeWitt, D.J. and Ramakrishnan, R.},\n",
        "  booktitle={22nd International Conference on Data Engineering (ICDE'06)},\n",
        "  title={Mondrian Multidimensional K-Anonymity},\n",
        "  year={2006},\n",
        "  pages={25-25},\n",
        "  keywords={Multidimensional systems;Protection;Greedy algorithms;Privacy;Approximation algorithms;Publishing;Demography;Public healthcare;Data security;Databases},\n",
        "  doi={10.1109/ICDE.2006.101}\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkflAHcFUODk"
      },
      "source": [
        "The hierarchies for the attributes workclass, education, marital-status and relationship are taken from the paper \"Clustering ofmixed-type data considering concept hierarchies:\n",
        "problem specification and algorithm\", S.Behzadi et. al.\n",
        "\n",
        "```\n",
        "\n",
        "@article{d7acd3875c184273b4e295ff3f116d3f,\n",
        "title = \"Clustering of mixed-type data considering concept hierarchies: problem specification and algorithm\",\n",
        "keywords = \"Information-theoretic clustering, Mixed-type data\",\n",
        "author = \"Sahar Behzadi and M{\\\"u}ller, {N. S.} and Claudia Plant and C. B{\\\"o}hm\",\n",
        "note = \"Publisher Copyright: {\\textcopyright} 2020, The Author(s).\",\n",
        "year = \"2020\",\n",
        "month = sep,\n",
        "doi = \"10.1007/s41060-020-00216-2\",\n",
        "language = \"English\",\n",
        "volume = \"10\",\n",
        "pages = \"233--248\",\n",
        "journal = \"International Journal of Data Science and Analytics\",\n",
        "issn = \"2364-415X\",\n",
        "publisher = \"Springer\",\n",
        "}\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owgyMBtkxVNr"
      },
      "outputs": [],
      "source": [
        "# define generalization hierarchies for the categorical attributes\n",
        "\n",
        "workclass_hierarchy = {\n",
        "    \"*\": {\n",
        "        \"Self-employed\": [' Self-emp-not-inc', ' Self-emp-inc'],\n",
        "        \"Government\": [' Federal-gov', ' Local-gov', ' State-gov'],\n",
        "        \"Other\": [' Private', ' Without-pay']\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "education_hierarchy = {\n",
        "    \"*\": {\n",
        "        \"High-school\": [' 1st-4th', ' 5th-6th', ' 7th-8th', ' 9th', ' 10th', ' 11th', ' 12th', ' HS-grad'],\n",
        "        \"undergrad\": [' Bachelors', ' Some-college', ' Prof-school', ' Assoc-voc', ' Assoc-acdm', ' Preschool'],\n",
        "        \"academia\": [' Masters', ' Doctorate']\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "marital_status_hierarchy = {\n",
        "    \"*\": {\n",
        "        \"married\": [' Married-civ-spouse', ' Married-spouse-absent', ' Married-AF-spouse'],\n",
        "        \"single\": [' Divorced', ' Separated', ' Widowed', ' Never-married']\n",
        "    }\n",
        "}\n",
        "\n",
        "relationship_hierarchy = {\n",
        "    \"*\": {\n",
        "        \"Family\": [\" Wife\", \" Own-child\", \" Husband\", \" Other-relative\"],\n",
        "        \"No Family\": [\" Unmarried\", \" Not-in-family\"]\n",
        "    }\n",
        "}\n",
        "occupation_hierarchy = {\n",
        "    \"Occupation\": {\n",
        "        \"White-Collar\": [\" Adm-clerical\", \" Exec-managerial\", \" Prof-specialty\", \" Sales\", \" Tech-support\"],\n",
        "        \"Blue-Collar\": [\" Handlers-cleaners\", \" Other-service\", \" Transport-moving\", \" Farming-fishing\",\n",
        "                        \" Machine-op-inspct\", \" Craft-repair\", \" Protective-serv\", \" Priv-house-serv\", \" Armed-Forces\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "native_country_hierarchy = {\n",
        "    \"Native Country\": {\n",
        "        \"North America\": [\" United-States\", \" Canada\", \" Outlying-US(Guam-USVI-etc)\", \" Puerto-Rico\"],\n",
        "        \"Central/South America\": [\" Cuba\", \" Jamaica\", \" Mexico\", \" Honduras\", \" Columbia\", \" Dominican-Republic\",\n",
        "            \" Ecuador\", \" El-Salvador\", \" Guatemala\", \" Haiti\", \" Peru\", \" Trinadad&Tobago\", \" Nicaragua\"],\n",
        "        \"Europe\": [\" England\", \" Germany\", \" Poland\", \" Portugal\", \" France\", \" Italy\", \" Scotland\", \" Yugoslavia\",\n",
        "            \" Greece\", \" Ireland\", \" Hungary\", \" Holand-Netherlands\"],\n",
        "        \"Asia\": [\" India\", \" Iran\", \" Philippines\", \" Cambodia\", \" Thailand\", \" Laos\", \" Taiwan\",\n",
        "            \" China\", \" South\", \" Japan\", \" Vietnam\", \" Hong\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "income = {\"income\": [\" <=50K\", \" >50K\"]}\n",
        "\n",
        "sex_hierarchy = {\"*\": [\" Male\", \" Female\"]}\n",
        "\n",
        "\"\"\"\n",
        "get hierarchy of an attribute\n",
        "\"\"\"\n",
        "\n",
        "def check_hierarchy(attribute):\n",
        "  if attribute == 'workclass':\n",
        "    return workclass_hierarchy\n",
        "  elif attribute == 'education':\n",
        "    return education_hierarchy\n",
        "  elif attribute == 'marital-status':\n",
        "    return marital_status_hierarchy\n",
        "  elif attribute == 'relationship':\n",
        "    return relationship_hierarchy\n",
        "  elif attribute == 'occupation':\n",
        "    return occupation_hierarchy\n",
        "  elif attribute == 'native-country':\n",
        "    return native_country_hierarchy\n",
        "  elif attribute == \"income\":\n",
        "    return income\n",
        "  elif attribute == \"sex\":\n",
        "    return sex_hierarchy\n",
        "  elif attribute == \"age\":\n",
        "    return age_hierarchy\n",
        "  elif attribute == \"fnlwgt\":\n",
        "    return fnlwgt_hierarchy\n",
        "  elif attribute == \"hours-per-week\":\n",
        "    return hours_per_week_hierarchy\n",
        "  elif attribute == \"capital-gain\":\n",
        "    return capital_gain_hierarchy\n",
        "  elif attribute == \"capital-loss\":\n",
        "    return capital_loss_hierarchy\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "\"\"\"\n",
        " defintion of helper functions\n",
        "\"\"\"\n",
        "\n",
        "# check, if dataset is l-diverse\n",
        "def is_l_diverse(dataset, attribute, l):\n",
        "  if dataset[attribute].nunique() >= l:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "# find middle of the partition and return middle element\n",
        "def find_middle(partition):\n",
        "\n",
        "    middle_index = len(partition) // 2\n",
        "    return partition[middle_index]\n",
        "\n",
        "# return quasi identifier with the most unique values\n",
        "def widest_qi(dataset, qis):\n",
        "  unique_values_qi = []\n",
        "\n",
        "  for qi in qis:\n",
        "    unique_values_qi.append((qi, dataset[qi].unique()))\n",
        "    sorted_qi = sorted(unique_values_qi, key=lambda x: len(x[1]), reverse=True)\n",
        "\n",
        "  return sorted_qi[0][0]\n",
        "\n",
        "\n",
        "\n",
        "# generalize categorical attribute to the next level in the generalization hierarchy\n",
        "def gen_to_higher_level(attributeVal, hierarchy):\n",
        "\n",
        "  for key, value in hierarchy.items():\n",
        "    if isinstance(value, dict) and attributeVal in list(value.keys()):\n",
        "        return key\n",
        "    elif isinstance(value, dict):\n",
        "      result = gen_to_higher_level(attributeVal, value)\n",
        "      if result != None:\n",
        "        return result\n",
        "    elif attributeVal in value if isinstance(value, list) else attributeVal == value:\n",
        "      return key\n",
        "  return attributeVal\n",
        "\n",
        "# take minimum(rounded down) and maximum (rounded up) value in numerical attribute and turn it into an interval\n",
        "def generalize_numeric(dataset, qi):\n",
        "  min_val, max_val = dataset[qi].min(), dataset[qi].max()\n",
        "  if min_val == max_val:\n",
        "    dataset[qi] = \"*\"\n",
        "  elif qi == \"age\":\n",
        "    min_val = min_val - min_val % 5\n",
        "    max_val = max_val + 5 - (max_val % 5)\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  elif qi == \"fnlwgt\":\n",
        "    min_val = min_val - min_val % 100000\n",
        "    max_val = max_val + 100000 - (max_val % 100000)\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  elif qi == \"hours-per-week\":\n",
        "    min_val = min_val - min_val % 5\n",
        "    max_val = max_val + 5 - (max_val % 5)\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  elif qi == \"capital-gain\":\n",
        "    min_val = min_val - min_val % 1000\n",
        "    max_val = max_val + 1000 - (max_val % 1000)\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  elif qi == \"capital-loss\":\n",
        "    min_val = min_val - min_val % 1000\n",
        "    max_val = max_val + 1000 - (max_val % 1000)\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\n",
        "def is_k_anonymous_categorical(partition, categorical_qis):\n",
        "\n",
        "    # Select only the categorical quasi-identifier columns\n",
        "    qi_data = partition[categorical_qis]\n",
        "\n",
        "    # Count the occurrences of unique combinations of categorical QI values\n",
        "    counts = qi_data.groupby(categorical_qis).size()\n",
        "\n",
        "    # Check if all counts are greater than or equal to k\n",
        "    # and explicitly return True or False\n",
        "    k = len(partition)\n",
        "    if (counts >= k).all():\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def refine_generalization(dataset, qi):\n",
        "# Work on a copy to avoid modifying the original\n",
        "\n",
        "    #for qi in qis:\n",
        "      #  if kind_of_attribute(qi) == \"categorical\":  # Numeric or categorical that has been generalized\n",
        "\n",
        "  hierarchy = check_hierarchy(qi)\n",
        "  if isinstance(hierarchy, list):\n",
        "    print(f\"Attribute: {qi}, Hierarchy Type: {type(hierarchy)}\")\n",
        "  if hierarchy is not None and isinstance(hierarchy, dict):\n",
        "    dataset[qi] = dataset[qi].apply(lambda x: gen_to_higher_level(x, hierarchy))\n",
        "\n",
        "  return dataset\n",
        "\n",
        "# make partition k-anonymous\n",
        "def ensure_k_anonymity_categorical(partition):\n",
        "  categorical_qis = ['Gender', 'family_history_with_overweight', 'FAVC',  'CAEC', 'SMOKE','SCC', 'CALC', 'MTRANS']\n",
        "  #[\"workclass\", \"education\", \"marital-status\", \"relationship\", \"occupation\", \"native-country\"]\n",
        "\n",
        "  generalized_partition = partition.copy()\n",
        "  while not is_k_anonymous_categorical(generalized_partition, categorical_qis):\n",
        "        # Find categorical columns with multiple values\n",
        "        columns_to_generalize = [\n",
        "            col for col in categorical_qis\n",
        "            if generalized_partition[col].nunique() > 1  # More than one unique value\n",
        "        ]\n",
        "\n",
        "        if not columns_to_generalize:\n",
        "            # If no columns can be generalized further, break the loop\n",
        "            break\n",
        "\n",
        "        # Generalize the first column in the list (minimal generalization)\n",
        "        column_to_generalize = columns_to_generalize[0]\n",
        "        hierarchy = check_hierarchy(column_to_generalize)\n",
        "\n",
        "        generalized_partition[column_to_generalize] = generalized_partition[column_to_generalize].apply(lambda x: gen_to_higher_level(x, hierarchy))\n",
        "\n",
        "  return generalized_partition\n",
        "\n",
        "\n",
        "def generalize(dataset,qis):\n",
        "  for qi in qis:\n",
        "    if kind_of_attribute(qi) == \"numeric\":\n",
        "      dataset = generalize_numeric(dataset, qi)\n",
        "    else:\n",
        "      dataset = refine_generalization(dataset, qi)\n",
        "\n",
        "  dataset = ensure_k_anonymity_categorical(dataset)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This recursive function checks for k-anonymity and l-diversity in each recursion\n",
        "step. The algorithm is based on the algorithm of LeFevre et. al., as stated above.\n",
        "\"\"\"\n",
        "\n",
        "def l_mondrian(dataset, k, qis, l, sensitive_attribute):\n",
        "    global recursive_call\n",
        "\n",
        "    if len(dataset) < 2 * k:\n",
        "      return generalize(dataset.copy(), qis)\n",
        "    if recursive_call == 0:\n",
        "      # picked age attribute first, for a more balanced split on this attribute\n",
        "      selected_qi = \"age\"\n",
        "    else:\n",
        "      selected_qi = widest_qi(dataset, qis)\n",
        "\n",
        "    # Handle numeric attributes\n",
        "    if dataset[selected_qi].dtype == \"int64\":\n",
        "\n",
        "        sorted_dataset = dataset.sort_values(by=selected_qi).copy()\n",
        "        middle_index = len(sorted_dataset) // 2\n",
        "        left_side = sorted_dataset.iloc[:middle_index].copy()\n",
        "        right_side = sorted_dataset.iloc[middle_index:].copy()\n",
        "\n",
        "    # Handle categorical attributes\n",
        "    elif dataset[selected_qi].dtype == \"object\":\n",
        "        print(selected_qi)\n",
        "        hierarchy = check_obesity_hierarchy(selected_qi)\n",
        "\n",
        "        dataset[selected_qi] = dataset[selected_qi].apply(lambda x: gen_to_higher_level(x, hierarchy))\n",
        "        sorted_partition = sorted(dataset[selected_qi].unique())\n",
        "        split_value = find_middle(sorted_partition)\n",
        "        group_left = sorted_partition[:sorted_partition.index(split_value)]\n",
        "        group_right = sorted_partition[sorted_partition.index(split_value):]\n",
        "        left_side = dataset[dataset[selected_qi].isin(group_left)].copy()\n",
        "        right_side = dataset[dataset[selected_qi].isin(group_right)].copy()\n",
        "\n",
        "    # Recursively apply l_mondrian on the partitions\n",
        "    if (len(left_side) >=k and is_l_diverse(left_side, sensitive_attribute, l)) and (len(right_side) >= k and is_l_diverse(right_side, sensitive_attribute, l)):\n",
        "      recursive_call += 1\n",
        "      partition_one = l_mondrian(left_side, k, qis, l, sensitive_attribute)\n",
        "      partition_two = l_mondrian(right_side, k, qis, l, sensitive_attribute)\n",
        "\n",
        "\n",
        "    else:\n",
        "      return generalize(dataset.copy(), qis)\n",
        "\n",
        "\n",
        "    # Concatenate the generalized partitions\n",
        "    return pd.concat([partition_one, partition_two])\n",
        "\n",
        "\n",
        " # test with k=100\n",
        "\n",
        "recursive_call = 0\n",
        "test_path = \"/content/drive/MyDrive/Datasets/adultsClean.csv\"\n",
        "test = pd.read_csv(test_path)\n",
        "#mondrian = l_mondrian(test, 100, [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"marital-status\",\"occupation\", \"relationship\", \"sex\", \"capital-gain\",\"capital-loss\", \"hours-per-week\",  \"native-country\"], 2, \"income\")\n",
        "#mondrian[[\"age\", \"workclass\",\"fnlwgt\", \"education\", \"marital-status\",\"occupation\", \"relationship\",\"sex\", \"hours-per-week\",  \"native-country\", \"income\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khAXchWLFD7K"
      },
      "source": [
        "# Computing Relaxed Functional Dependencies\n",
        "\n",
        "THe RFDs are computed with the DOMINO algorithm, as proposed in \"Discovering Relaxed Functional Dependencies Based on Multi-Attribute Dominance\", by L.Caruccio et. al.\n",
        "```\n",
        "\n",
        "  author={Caruccio, Loredana and Deufemia, Vincenzo and Naumann, Felix and Polese, Giuseppe},\n",
        "  journal={IEEE Transactions on Knowledge and Data Engineering},\n",
        "  title={Discovering Relaxed Functional Dependencies Based on Multi-Attribute Dominance},\n",
        "  year={2021},\n",
        "  volume={33},\n",
        "  number={9},\n",
        "  pages={3212-3228},\n",
        "  keywords={Complexity theory;Approximation algorithms;Big Data;Distributed databases;Semantics;Lakes;Functional dependencies;data profiling;data cleansing},\n",
        "  doi={10.1109/TKDE.2020.2967722}\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya_cmkJ3M1Gs",
        "outputId": "0d1c60ae-351a-4080-9526-7d14a02b9303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'thresholds': [1, 1, 1, 1], 'cc': (1, 2, 10, 11)}, {'thresholds': [2, 1, 1, 4], 'cc': (2, 4, 5, 10)}, {'thresholds': [4, 2, 1, 4], 'cc': (0, 2, 3, 8)}, {'thresholds': [1, 1, 1, 1], 'cc': (1, 2, 4, 5)}, {'thresholds': [2, 1, 4, 1], 'cc': (2, 3, 10, 11)}, {'thresholds': [3, 1, 1, 4], 'cc': (0, 1, 5, 10)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 2, 3, 11)}, {'thresholds': [2, 1, 1, 4], 'cc': (2, 3, 5, 10)}, {'thresholds': [4, 1, 1, 4], 'cc': (0, 1, 5, 8)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 1, 2, 4)}, {'thresholds': [2, 1, 1, 1], 'cc': (2, 3, 5, 9)}, {'thresholds': [4, 2, 1, 1], 'cc': (0, 2, 3, 6)}, {'thresholds': [4, 2, 4, 1], 'cc': (0, 2, 10, 11)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 1, 2, 11)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 2, 3, 4)}, {'thresholds': [2, 1, 1, 4], 'cc': (2, 3, 5, 10)}, {'thresholds': [1, 1, 1, 1], 'cc': (1, 2, 5, 9)}, {'thresholds': [1, 3, 4, 1], 'cc': (1, 8, 10, 11)}, {'thresholds': [1, 1, 1, 1], 'cc': (1, 2, 3, 4)}, {'thresholds': [1, 2, 1, 1], 'cc': (1, 2, 3, 5)}, {'thresholds': [1, 2, 1, 1], 'cc': (1, 2, 3, 5)}, {'thresholds': [4, 1, 1, 4], 'cc': (0, 3, 5, 8)}, {'thresholds': [1, 1, 1, 2], 'cc': (1, 2, 4, 9)}, {'thresholds': [1, 1, 1, 1], 'cc': (1, 2, 5, 11)}, {'thresholds': [2, 1, 1, 3], 'cc': (2, 3, 5, 10)}, {'thresholds': [1, 1, 1, 3], 'cc': (3, 4, 5, 10)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 1, 2, 4)}, {'thresholds': [1, 1, 1, 2], 'cc': (0, 2, 5, 10)}, {'thresholds': [4, 1, 3, 4], 'cc': (0, 1, 8, 10)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 1, 2, 4)}, {'thresholds': [1, 1, 1, 2], 'cc': (0, 2, 5, 10)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 3, 4, 5)}, {'thresholds': [4, 1, 4, 1], 'cc': (0, 2, 10, 11)}, {'thresholds': [2, 1, 1, 1], 'cc': (2, 3, 5, 9)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 1, 2, 4)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 2, 5, 10)}, {'thresholds': [4, 1, 1, 3], 'cc': (0, 4, 5, 10)}, {'thresholds': [4, 2, 1, 4], 'cc': (0, 2, 5, 10)}, {'thresholds': [2, 1, 1, 4], 'cc': (2, 3, 5, 10)}, {'thresholds': [4, 2, 1, 3], 'cc': (0, 2, 5, 10)}, {'thresholds': [4, 2, 1, 1], 'cc': (0, 2, 3, 4)}, {'thresholds': [2, 1, 1, 4], 'cc': (2, 3, 5, 10)}, {'thresholds': [1, 1, 4, 1], 'cc': (4, 5, 10, 11)}, {'thresholds': [1, 1, 3, 1], 'cc': (1, 2, 8, 11)}, {'thresholds': [1, 1, 1, 1], 'cc': (1, 2, 3, 4)}, {'thresholds': [4, 2, 4, 1], 'cc': (0, 2, 10, 11)}, {'thresholds': [4, 2, 1, 1], 'cc': (0, 2, 3, 4)}, {'thresholds': [4, 2, 1, 1], 'cc': (0, 2, 3, 5)}, {'thresholds': [4, 2, 1, 2], 'cc': (0, 2, 6, 10)}, {'thresholds': [4, 3, 2, 3], 'cc': (0, 8, 9, 10)}, {'thresholds': [4, 1, 1, 4], 'cc': (0, 3, 5, 10)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 2, 3, 4)}, {'thresholds': [2, 1, 1, 4], 'cc': (2, 3, 4, 10)}, {'thresholds': [4, 2, 1, 1], 'cc': (0, 2, 3, 9)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 3, 4, 10)}, {'thresholds': [1, 2, 1, 1], 'cc': (1, 2, 4, 5)}, {'thresholds': [1, 1, 1, 1], 'cc': (0, 2, 5, 6)}, {'thresholds': [4, 1, 3, 1], 'cc': (0, 3, 8, 11)}, {'thresholds': [2, 1, 1, 4], 'cc': (2, 3, 4, 10)}, {'thresholds': [4, 2, 1, 1], 'cc': (0, 2, 3, 9)}, {'thresholds': [2, 1, 1, 5], 'cc': (2, 3, 5, 10)}, {'thresholds': [2, 1, 1, 4], 'cc': (2, 3, 4, 10)}, {'thresholds': [1, 1, 1, 1], 'cc': (1, 3, 5, 8)}, {'thresholds': [1, 1, 1, 2], 'cc': (0, 2, 4, 10)}, {'thresholds': [1, 1, 1, 1], 'cc': (1, 2, 5, 11)}, {'thresholds': [2, 1, 1, 4], 'cc': (2, 3, 5, 8)}, {'thresholds': [4, 1, 1, 4], 'cc': (0, 4, 5, 10)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 3, 4, 5)}, {'thresholds': [4, 2, 1, 1], 'cc': (0, 2, 3, 9)}, {'thresholds': [2, 1, 1, 4], 'cc': (2, 3, 5, 10)}, {'thresholds': [1, 1, 1, 1], 'cc': (1, 2, 4, 9)}, {'thresholds': [1, 2, 1, 1], 'cc': (1, 2, 4, 5)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 3, 5, 11)}, {'thresholds': [1, 1, 1, 1], 'cc': (2, 4, 5, 10)}, {'thresholds': [4, 2, 1, 1], 'cc': (0, 2, 3, 11)}, {'thresholds': [2, 1, 2, 4], 'cc': (2, 4, 9, 10)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 1, 2, 5)}, {'thresholds': [2, 1, 1, 5], 'cc': (2, 3, 5, 10)}, {'thresholds': [1, 1, 1, 1], 'cc': (1, 2, 3, 4)}, {'thresholds': [4, 2, 1, 2], 'cc': (0, 2, 3, 8)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 2, 3, 4)}, {'thresholds': [2, 1, 1, 1], 'cc': (2, 3, 5, 11)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 3, 4, 5)}, {'thresholds': [4, 1, 4, 1], 'cc': (0, 3, 10, 11)}, {'thresholds': [4, 2, 1, 1], 'cc': (0, 2, 3, 6)}, {'thresholds': [4, 1, 4, 1], 'cc': (0, 3, 10, 11)}, {'thresholds': [2, 1, 1, 4], 'cc': (2, 3, 5, 10)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 2, 3, 4)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 3, 4, 5)}, {'thresholds': [4, 1, 1, 4], 'cc': (0, 3, 4, 10)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 2, 3, 11)}, {'thresholds': [1, 1, 1, 1], 'cc': (1, 2, 3, 4)}, {'thresholds': [1, 1, 1, 1], 'cc': (1, 2, 5, 11)}, {'thresholds': [4, 2, 1, 4], 'cc': (0, 2, 3, 8)}, {'thresholds': [4, 2, 1, 1], 'cc': (0, 2, 3, 5)}, {'thresholds': [4, 1, 1, 1], 'cc': (0, 3, 4, 5)}, {'thresholds': [4, 1, 2, 1], 'cc': (0, 1, 2, 5)}, {'thresholds': [4, 1, 1, 3], 'cc': (0, 3, 4, 10)}, {'thresholds': [2, 1, 3, 4], 'cc': (2, 3, 8, 10)}, {'thresholds': [4, 2, 1, 1], 'cc': (0, 2, 3, 4)}]\n",
            "[{'thresholds': [4, 2, 1], 'cc': (0, 2, 4)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 1, 4)}, {'thresholds': [1, 1, 2], 'cc': (4, 5, 9)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 5)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 5)}, {'thresholds': [3, 1, 1], 'cc': (0, 4, 5)}, {'thresholds': [2, 1, 3], 'cc': (2, 3, 8)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 3)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 4)}, {'thresholds': [2, 1, 4], 'cc': (2, 5, 10)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 4)}, {'thresholds': [1, 1, 4], 'cc': (3, 4, 10)}, {'thresholds': [4, 1, 1], 'cc': (0, 1, 4)}, {'thresholds': [1, 1, 3], 'cc': (3, 5, 10)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 1, 4)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 5)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 4)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 4)}, {'thresholds': [], 'cc': (1, 4, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 11)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 5)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 4)}, {'thresholds': [1, 1, 4], 'cc': (1, 2, 10)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 4)}, {'thresholds': [4, 1, 3], 'cc': (0, 4, 8)}, {'thresholds': [1, 2, 1], 'cc': (1, 2, 6)}, {'thresholds': [4, 1, 1], 'cc': (0, 1, 9)}, {'thresholds': [4, 1, 1], 'cc': (0, 1, 9)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 4)}, {'thresholds': [1, 1, 1], 'cc': (1, 3, 5)}, {'thresholds': [4, 1, 3], 'cc': (0, 3, 8)}, {'thresholds': [1, 1, 1], 'cc': (1, 4, 5)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 4)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 8)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 5)}, {'thresholds': [2, 1, 4], 'cc': (2, 3, 10)}, {'thresholds': [1, 2, 1], 'cc': (1, 2, 5)}, {'thresholds': [2, 1, 3], 'cc': (2, 3, 8)}, {'thresholds': [4, 1, 1], 'cc': (0, 4, 5)}, {'thresholds': [4, 2, 4], 'cc': (0, 2, 10)}, {'thresholds': [2, 1, 4], 'cc': (2, 5, 10)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 4)}, {'thresholds': [1, 1, 1], 'cc': (1, 3, 5)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 2, 10)}, {'thresholds': [1, 1, 1], 'cc': (4, 5, 6)}, {'thresholds': [1, 3, 1], 'cc': (4, 10, 11)}, {'thresholds': [1, 1, 1], 'cc': (1, 5, 6)}, {'thresholds': [1, 1, 3], 'cc': (4, 5, 8)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 4)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 5)}, {'thresholds': [1, 1, 2], 'cc': (3, 4, 10)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 9)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 5)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 5)}, {'thresholds': [2, 1, 4], 'cc': (2, 5, 10)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 6)}, {'thresholds': [1, 1, 4], 'cc': (3, 4, 10)}, {'thresholds': [1, 1, 1], 'cc': (4, 5, 6)}, {'thresholds': [4, 1, 3], 'cc': (0, 4, 8)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 10)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 11)}, {'thresholds': [1, 3, 1], 'cc': (4, 10, 11)}, {'thresholds': [4, 1, 1], 'cc': (0, 1, 6)}, {'thresholds': [4, 2, 4], 'cc': (0, 2, 10)}, {'thresholds': [1, 1, 1], 'cc': (4, 5, 6)}, {'thresholds': [4, 1, 4], 'cc': (0, 4, 10)}, {'thresholds': [1, 1, 1], 'cc': (1, 2, 3)}, {'thresholds': [4, 1, 4], 'cc': (0, 5, 10)}, {'thresholds': [1, 1, 4], 'cc': (3, 4, 10)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 5)}, {'thresholds': [], 'cc': (0, 3, 5)}, {'thresholds': [4, 1, 4], 'cc': (0, 5, 10)}, {'thresholds': [4, 1, 1], 'cc': (0, 5, 10)}, {'thresholds': [1, 1, 1], 'cc': (1, 3, 4)}, {'thresholds': [2, 1, 4], 'cc': (2, 3, 10)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 9)}, {'thresholds': [1, 1, 4], 'cc': (3, 5, 10)}, {'thresholds': [1, 1, 3], 'cc': (4, 5, 8)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 5)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 5)}, {'thresholds': [1, 2, 1], 'cc': (1, 2, 5)}, {'thresholds': [1, 1, 1], 'cc': (4, 5, 6)}, {'thresholds': [1, 1, 3], 'cc': (4, 5, 10)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 4)}, {'thresholds': [2, 4, 1], 'cc': (2, 10, 11)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 4)}, {'thresholds': [1, 1, 1], 'cc': (1, 2, 11)}, {'thresholds': [2, 1, 4], 'cc': (2, 3, 10)}, {'thresholds': [4, 1, 1], 'cc': (0, 1, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 4, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 1, 5)}, {'thresholds': [1, 2, 1], 'cc': (1, 2, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 1, 5)}, {'thresholds': [4, 2, 2], 'cc': (0, 2, 10)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 4)}, {'thresholds': [4, 1, 1], 'cc': (0, 2, 3)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 4)}, {'thresholds': [1, 1, 2], 'cc': (1, 3, 10)}, {'thresholds': [1, 1, 1], 'cc': (2, 3, 5)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 5)}, {'thresholds': [2, 1, 4], 'cc': (2, 5, 10)}, {'thresholds': [1, 1, 1], 'cc': (1, 5, 8)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 11)}, {'thresholds': [1, 2, 1], 'cc': (1, 2, 5)}, {'thresholds': [4, 2, 4], 'cc': (0, 2, 10)}, {'thresholds': [4, 1, 3], 'cc': (0, 6, 10)}, {'thresholds': [1, 1, 1], 'cc': (1, 2, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 1, 5)}, {'thresholds': [4, 1, 3], 'cc': (0, 4, 8)}, {'thresholds': [4, 1, 1], 'cc': (0, 2, 3)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 3)}, {'thresholds': [1, 2, 1], 'cc': (1, 2, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 11)}, {'thresholds': [4, 1, 1], 'cc': (0, 4, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 9)}, {'thresholds': [1, 1, 2], 'cc': (1, 3, 8)}, {'thresholds': [4, 2, 4], 'cc': (0, 2, 10)}, {'thresholds': [3, 1, 1], 'cc': (0, 3, 5)}, {'thresholds': [4, 1, 2], 'cc': (0, 4, 10)}, {'thresholds': [1, 1, 1], 'cc': (1, 2, 5)}, {'thresholds': [1, 1, 2], 'cc': (1, 5, 10)}, {'thresholds': [4, 1, 3], 'cc': (0, 4, 10)}, {'thresholds': [1, 1, 4], 'cc': (3, 4, 10)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 5)}, {'thresholds': [1, 1, 4], 'cc': (1, 3, 10)}, {'thresholds': [1, 1, 4], 'cc': (3, 5, 10)}, {'thresholds': [4, 1, 3], 'cc': (0, 5, 10)}, {'thresholds': [1, 2, 1], 'cc': (1, 2, 5)}, {'thresholds': [1, 1, 1], 'cc': (1, 2, 3)}, {'thresholds': [3, 1, 1], 'cc': (2, 4, 5)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 10)}, {'thresholds': [1, 1, 4], 'cc': (5, 6, 10)}, {'thresholds': [2, 1, 2], 'cc': (2, 3, 8)}, {'thresholds': [1, 1, 4], 'cc': (3, 5, 10)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 11)}, {'thresholds': [4, 1, 1], 'cc': (0, 2, 3)}, {'thresholds': [2, 1, 3], 'cc': (2, 3, 10)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 4)}, {'thresholds': [2, 1, 4], 'cc': (2, 3, 10)}, {'thresholds': [1, 1, 2], 'cc': (3, 4, 10)}, {'thresholds': [4, 1, 1], 'cc': (0, 4, 9)}, {'thresholds': [1, 1, 4], 'cc': (1, 5, 10)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 4)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 4)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 3)}, {'thresholds': [4, 1, 1], 'cc': (0, 4, 5)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 6)}, {'thresholds': [1, 1, 1], 'cc': (2, 3, 5)}, {'thresholds': [1, 1, 1], 'cc': (1, 3, 4)}, {'thresholds': [], 'cc': (2, 6, 7)}, {'thresholds': [1, 1, 4], 'cc': (3, 4, 10)}, {'thresholds': [2, 1, 2], 'cc': (2, 5, 10)}, {'thresholds': [4, 1, 4], 'cc': (0, 5, 10)}, {'thresholds': [2, 1, 4], 'cc': (2, 6, 10)}, {'thresholds': [2, 3, 4], 'cc': (2, 8, 10)}, {'thresholds': [4, 1, 1], 'cc': (0, 2, 4)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 4)}, {'thresholds': [1, 1, 4], 'cc': (4, 6, 10)}, {'thresholds': [4, 1, 3], 'cc': (0, 3, 8)}, {'thresholds': [1, 1, 4], 'cc': (1, 5, 10)}, {'thresholds': [1, 1, 1], 'cc': (1, 3, 5)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 4)}, {'thresholds': [1, 1, 3], 'cc': (3, 5, 10)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 4)}, {'thresholds': [1, 1, 4], 'cc': (1, 3, 10)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 3)}, {'thresholds': [1, 1, 1], 'cc': (2, 3, 10)}, {'thresholds': [1, 1, 1], 'cc': (0, 3, 5)}, {'thresholds': [1, 1, 4], 'cc': (3, 4, 10)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 6)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 5)}, {'thresholds': [2, 2, 4], 'cc': (2, 9, 10)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 10)}, {'thresholds': [4, 1, 2], 'cc': (0, 1, 2)}, {'thresholds': [1, 1, 1], 'cc': (1, 2, 5)}, {'thresholds': [2, 1, 1], 'cc': (2, 3, 6)}, {'thresholds': [1, 2, 1], 'cc': (1, 2, 3)}, {'thresholds': [4, 1, 3], 'cc': (0, 5, 10)}, {'thresholds': [4, 1, 2], 'cc': (0, 5, 10)}, {'thresholds': [2, 1, 4], 'cc': (2, 5, 10)}, {'thresholds': [1, 1, 1], 'cc': (1, 5, 9)}, {'thresholds': [2, 1, 1], 'cc': (0, 4, 5)}, {'thresholds': [1, 1, 2], 'cc': (1, 5, 8)}, {'thresholds': [4, 1, 1], 'cc': (0, 3, 11)}, {'thresholds': [2, 1, 4], 'cc': (2, 3, 10)}, {'thresholds': [4, 2, 3], 'cc': (0, 2, 8)}, {'thresholds': [1, 1, 4], 'cc': (3, 4, 10)}, {'thresholds': [4, 1, 4], 'cc': (0, 6, 10)}, {'thresholds': [4, 2, 2], 'cc': (0, 2, 8)}, {'thresholds': [2, 1, 5], 'cc': (2, 3, 10)}, {'thresholds': [4, 2, 4], 'cc': (0, 2, 10)}, {'thresholds': [4, 3, 2], 'cc': (0, 8, 10)}, {'thresholds': [1, 1, 2], 'cc': (0, 2, 10)}, {'thresholds': [4, 2, 1], 'cc': (0, 2, 3)}, {'thresholds': [4, 1, 3], 'cc': (0, 4, 8)}]\n"
          ]
        }
      ],
      "source": [
        "# helper function to create generalization hierarchies of numerical attributes\n",
        "\n",
        "def create_gen_hierarchy(values, x):\n",
        "    values = sorted(values)\n",
        "\n",
        "    # This function creates the first level of intervals (first generalization level)\n",
        "    def generate_intervals(values, x):\n",
        "        # round minimum value down\n",
        "        values[0]  = values[0] - values[0] % x\n",
        "        # round maximum value up\n",
        "        values[-1] = values[-1] + x - (values[-1] - values[0]) % x\n",
        "        min_value = values[0]\n",
        "        max_value = values[-1]\n",
        "\n",
        "        # Start intervals from the minimum value and increase in steps of x\n",
        "        intervals = []\n",
        "        start = min_value\n",
        "        while start < max_value:\n",
        "            end = start + x\n",
        "            # collect the interval\n",
        "            intervals.append((start, min(end, max_value)))\n",
        "            start += x\n",
        "\n",
        "        return intervals\n",
        "\n",
        "    # build the generalization hierarchy\n",
        "    def build_hierarchy(values, x):\n",
        "        hierarchy = []\n",
        "        level = generate_intervals(values, x)\n",
        "        hierarchy.append(level)\n",
        "\n",
        "        # continue merging intervals until a single interval remains\n",
        "        while len(level) > 1:\n",
        "            # create the next level by merging adjacent intervals\n",
        "            next_level = []\n",
        "            for i in range(0, len(level), 2):\n",
        "                if i + 1 < len(level):\n",
        "                    merge = (level[i][0], level[i+1][1])\n",
        "                    next_level.append(merge)\n",
        "\n",
        "                else:\n",
        "                    next_level.append(level[i])\n",
        "\n",
        "            level = next_level\n",
        "            hierarchy.append(level)\n",
        "\n",
        "        return hierarchy\n",
        "\n",
        "\n",
        "    return build_hierarchy(values, x)\n",
        "\n",
        "# create hierarchies\n",
        "age_values = test[\"age\"].unique()\n",
        "age_hierarchy = create_gen_hierarchy(age_values, 5)\n",
        "fnlwgt_values = test[\"fnlwgt\"].unique()\n",
        "fnlwgt_hierarchy = create_gen_hierarchy(fnlwgt_values, 100000)\n",
        "hours_per_week_values = test[\"hours-per-week\"].unique()\n",
        "hours_per_week_hierarchy = create_gen_hierarchy(hours_per_week_values, 5)\n",
        "capital_gain_values = test[\"capital-gain\"].unique()\n",
        "capital_gain_hierarchy = create_gen_hierarchy(capital_gain_values, 1000)\n",
        "capital_loss_values = test[\"capital-loss\"].unique()\n",
        "capital_loss_hierarchy = create_gen_hierarchy(capital_loss_values, 1000)\n",
        "\n",
        "\n",
        "# compute after how many levels two numeric attribute values end up in the same interval\n",
        "def generalization_distance_numeric(x,y, genHierarchy):\n",
        "  if x == y:\n",
        "    return 0\n",
        "  else:\n",
        "    height = 0\n",
        "    interval_One = ()\n",
        "    interval_Two = ()\n",
        "    for level in genHierarchy:\n",
        "      for interval in level:\n",
        "        if x >= interval[0] and x <= interval[1]:\n",
        "          interval_One = interval\n",
        "        if y >= interval[0] and y <= interval[1]:\n",
        "          interval_Two = interval\n",
        "      height += 1\n",
        "      if interval_One == interval_Two:\n",
        "        break\n",
        "  return height\n",
        "\n",
        "\n",
        "# compute after how many levels two categoric attribute values end up in the same interval\n",
        "def generalization_distance_categoric(valOne, valTwo, hierarchy):\n",
        "  if valOne == valTwo:\n",
        "    return 0\n",
        "  else:\n",
        "    height = 0\n",
        "    keyOne = None\n",
        "    keyTwo = None\n",
        "\n",
        "    for key, value in hierarchy.items():\n",
        "\n",
        "      if isinstance(value, dict):\n",
        "          for inner_key, inner_value in value.items():\n",
        "              if valOne in inner_value:\n",
        "                  keyOne = inner_key\n",
        "              if valTwo in inner_value:\n",
        "                  keyTwo = inner_key\n",
        "\n",
        "              if keyOne == keyTwo and keyOne is not None:\n",
        "                  height += 1\n",
        "                  break\n",
        "              elif keyOne is not None and keyTwo is not None:\n",
        "                  height +=1\n",
        "                  break\n",
        "      elif isinstance(value, list):\n",
        "          if valOne in value and valTwo in value:\n",
        "              height += 1\n",
        "              break\n",
        "          elif valOne in value or valTwo in value:\n",
        "            height +=1\n",
        "            break\n",
        "\n",
        "      if keyOne is not None and keyTwo is not None and keyOne != keyTwo:\n",
        "          return height + 1\n",
        "\n",
        "\n",
        "    return height\n",
        "\n",
        "\n",
        "########################### End of helper functions ############################\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The following functions implement the steps/algorithms mentioned in the paper.\n",
        "\"\"\"\n",
        "\n",
        "############################### Step 3.1: ######################################\n",
        "\n",
        "#Pairwise computation of the distance vectors between entries in given dataframe\n",
        "\n",
        "\n",
        "\n",
        "distance_data = []\n",
        "\n",
        "\n",
        "# create distance matrix with generalization distance as metric\n",
        "def compute_delta_df(dataset, num_rows, qi_columns, sensitive_attribute):\n",
        "  for i in range(num_rows):\n",
        "    for j in range(i + 1, num_rows):\n",
        "        row_distances = {}\n",
        "        for column in dataset.columns:\n",
        "            if column in qi_columns:\n",
        "                if dataset[column].dtype == \"int64\":\n",
        "                    distance = generalization_distance_numeric(dataset.loc[i, column], dataset.loc[j, column], check_hierarchy(column))\n",
        "                else:\n",
        "                    distance = generalization_distance_categoric(dataset.loc[i, column], dataset.loc[j, column], check_hierarchy(column))\n",
        "                row_distances[column] = distance\n",
        "            else:\n",
        "                row_distances[column] = None\n",
        "        distance_data.append(row_distances)\n",
        "  distance_df = pd.DataFrame(distance_data)\n",
        "  return distance_df.sort_values(by=sensitive_attribute)\n",
        "\n",
        "\n",
        "\n",
        "# test\n",
        "testDM = pd.read_csv(\"/content/drive/MyDrive/Datasets/adultDM.csv\")\n",
        "sample_size = 500\n",
        "testDM_sample = testDM.head(sample_size)\n",
        "test_rows = len(testDM_sample)\n",
        "\n",
        "\n",
        "test_columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"marital-status\", \"occupation\",\n",
        "                \"relationship\", \"sex\", \"capital-gain\", \"capital-loss\",  \"hours-per-week\",\n",
        "                \"native-country\", \"income\"]\n",
        "\n",
        "#testDelta = compute_delta_df(testDM_sample, test_rows, test_columns, \"income\")\n",
        "\n",
        "\n",
        "# save for later use\n",
        "#with open('/content/drive/MyDrive/pickledData/btestDelta.pickle', 'wb') as f:\n",
        " #   pickle.dump(testDelta, f)\n",
        "\n",
        "\n",
        "#display(testDelta.head(100))\n",
        "################################ Step 3.2 ######################################\n",
        "\n",
        "# Constructing T_beta relations.\n",
        "# T_beta contains all distance vectors that to not dominate each other with\n",
        "# sensitive_attribute values bigger than beta.\n",
        "\n",
        "\n",
        "# check, if vector v dominates any of the vectors in matrix T\n",
        "def dominates(v, T):\n",
        "    for t in T:\n",
        "      if (v >= t).all() and (v > t).any():\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "# Remove vectors t from T that dominate w\n",
        "def remove_dominated(w, T):\n",
        "    return [t for t in T if not ((t >= w).all() and (t > w).any())]\n",
        "\n",
        "\n",
        "# Create T_beta relations as described in the paper (Algorithm 2)\n",
        "def create_T_relations(df, S):\n",
        "  T = []\n",
        "  T_beta = []\n",
        "  beta = df.iloc[0][S]\n",
        "  beta_values = []\n",
        "  beta_values.append(beta)\n",
        "  sorted_df = df.sort_values(by=S, ascending=False)\n",
        "  for index, row in sorted_df.iterrows():\n",
        "    w = row.values\n",
        "\n",
        "    if w[-1] not in beta_values:\n",
        "      beta = w[-1]\n",
        "      beta_values.append(beta)\n",
        "      T.append(T_beta)\n",
        "      T_beta = []\n",
        "\n",
        "    if not dominates(w, T_beta):\n",
        "      T_beta.append(w)\n",
        "      remove_dominated(w, T_beta)\n",
        "\n",
        "  T.append(T_beta)\n",
        "\n",
        "  return T\n",
        "\n",
        "\n",
        "################################# Step 3.3 #####################################\n",
        "\n",
        "# Generating LHS candidates\n",
        "\n",
        "# compute difference matrix of vector w in T_beta\n",
        "def difference_matrix(T_beta, w):\n",
        "  DM = []\n",
        "  for v in T_beta:\n",
        "    if not np.array_equal(v, w):\n",
        "      difference = np.subtract(w,v)\n",
        "      # removing the sensitive attribute, since it will not be needed in the DM\n",
        "      DM.append(difference[:-1])\n",
        "  return DM\n",
        "\n",
        "# check, if given column combination X is a viable candidate (Definition 7)\n",
        "def is_viable(DM, X):\n",
        "    for row in DM:\n",
        "        is_admissible = False\n",
        "\n",
        "        for index in X:\n",
        "            if row[index] < 0:\n",
        "                is_admissible = True\n",
        "                break\n",
        "\n",
        "        if not is_admissible:\n",
        "            all_zeros = True\n",
        "            for index in X:\n",
        "                if row[index] != 0:\n",
        "                    all_zeros = False\n",
        "                    break\n",
        "            if not all_zeros:\n",
        "                # If not all zeros and no negative value, row is not admissible\n",
        "\n",
        "                return False\n",
        "\n",
        "    # If all rows are admissible, return True\n",
        "    return True\n",
        "\n",
        "\n",
        "\n",
        "#Algorithm 3 in Paper\n",
        "def create_lhs_candidates(T_beta, w):\n",
        "  L = 1\n",
        "  levelAttr = []\n",
        "  LHS = set()\n",
        "\n",
        "  DM = difference_matrix(T_beta, w)\n",
        "  num_attributes = len(w) - 1\n",
        "\n",
        "  while L <= num_attributes:\n",
        "    # generate column combinations\n",
        "    levelAttr = list(itertools.combinations(range(num_attributes), L))\n",
        "\n",
        "    new_levelAttr = []\n",
        "    # iterate over all combinations (sub-vectors) of size L\n",
        "    for X in levelAttr:\n",
        "      # Check if the length of the combination is less than or equal to 4 before checking viability\n",
        "      if len(X) <= 4 and is_viable(DM,X):\n",
        "        LHS.add(tuple(X))  # Add the sub-vector X to LHS if viable and length is within limit\n",
        "\n",
        "    L += 1\n",
        "\n",
        "  filtered_lhs = {tuple for tuple in LHS if len(tuple)>1}\n",
        "  return filtered_lhs\n",
        "\n",
        "################################ Step 3.4 ######################################\n",
        "\n",
        "# Determining threshold values of minimal RFDcs\n",
        "\n",
        "\n",
        "def find_min(TB_relation, m_i, sk, i,  j, thrs, w):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        TB_relation: The T_B relation (list of distance vectors).\n",
        "        m_j: The current value for the distance vector being evaluated (corresponding to attribute j).\n",
        "        sk: The current distance vector (LHS combination being evaluated).\n",
        "        i: The index of the attribute being evaluated.\n",
        "        thrs: The current list of thresholds.\n",
        "    Returns:\n",
        "        The minimum valid threshold value for the attribute X_j, or -1 if no valid value is found.\n",
        "    \"\"\"\n",
        "    # initialize with a large number to find the minimum valid threshold\n",
        "    min_value = float('inf')\n",
        "    for row in TB_relation:\n",
        "      if not np.array_equal(row, w) and j != i:\n",
        "        p_j = row[sk[j]]\n",
        "\n",
        "        # Step 1 and 2: p_j > d_j and p_i < d_i\n",
        "        if p_j <= w[sk[j]] and row[sk[i]] >= m_i:\n",
        "            continue  # Skip this row if p_j <= m_j\n",
        "\n",
        "        # Assume the row is valid unless proven otherwise\n",
        "        valid_row = True\n",
        "\n",
        "\n",
        "        # Step 3: all previous attributes satisfy their thresholds\n",
        "        for e in range(j):\n",
        "          if e==i:\n",
        "            continue\n",
        "          elif thrs[e] is not None and row[sk[e]] > thrs[e]:\n",
        "              valid_row = False\n",
        "              break\n",
        "\n",
        "        if not valid_row:\n",
        "            continue\n",
        "        valid_column = True\n",
        "\n",
        "        # Step 4: Check remaining attributes after j\n",
        "        for e in range(j + 1, len(sk)):\n",
        "          if not any(lst[e] >= row[e] for lst in TB_relation):\n",
        "            valid_column = False\n",
        "        if valid_row and valid_column:\n",
        "            min_value = min(min_value, p_j)\n",
        "            break\n",
        "\n",
        "    # Return the minimal valid value or -1 if no valid value is found\n",
        "    return min_value if min_value != float('inf') and min_value > 1 else -1\n",
        "\n",
        "\n",
        "# Helper functions\n",
        "def get_vector(w, cc):\n",
        "    return [i for i in cc]\n",
        "\n",
        "#Create the RFDc based on the given thresholds and their column combinations cc\n",
        "def create_rfdc(thrs, cc):\n",
        "    return {\"thresholds\": thrs, \"cc\": cc}\n",
        "\n",
        "\n",
        "\n",
        "# Algorithm 4 in paper\n",
        "def determine_thresholds(TB_relation, w, LHS):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        TB_relation: List of distance vectors.\n",
        "        w: A distance vector w in TB_relation.\n",
        "        LHS: Set of viable column combinations.\n",
        "    Returns:\n",
        "        List of discovered RFDcs.\n",
        "    \"\"\"\n",
        "    admissible = False\n",
        "    RFDcs = []\n",
        "\n",
        "    # iterate over all LHS combinations\n",
        "    for cc in LHS:\n",
        "      # get indices of distance vector for the current LHS combination\n",
        "        sk = get_vector(w, cc)\n",
        "\n",
        "      # get values of the indices in sk\n",
        "        check_lhs = []\n",
        "        for k in sk:\n",
        "          check_lhs.append(w[k])\n",
        "\n",
        "      # values of LHS thresholds are not allowed to equal 0\n",
        "        if not all(x > 0 for x in check_lhs):\n",
        "          continue\n",
        "\n",
        "        thrs = []\n",
        "\n",
        "        # Loop through the columns of sk (LHS combination)\n",
        "        for i in range(len(sk)):\n",
        "            m_i = w[sk[i]]\n",
        "\n",
        "            if m_i > 1:\n",
        "              admissible = True\n",
        "              thrs = [None] * len(sk)\n",
        "              # rule for epsilon step of 1\n",
        "              thrs[i] = (m_i - 1)\n",
        "              for j in range(len(sk)):\n",
        "                if i == 0:\n",
        "                  j +=1\n",
        "                min_thrs = find_min(TB_relation, m_i, sk, i, j, thrs, w)\n",
        "\n",
        "                if min_thrs != -1:\n",
        "                  thrs[j] = (min_thrs - 1)\n",
        "\n",
        "                  if all(alpha != None for alpha in thrs):\n",
        "                    break\n",
        "                else:\n",
        "\n",
        "                  admissible = False\n",
        "                  break\n",
        "\n",
        "        # If admissible, create RFD and add it to the list of RFDs.\n",
        "        if admissible:\n",
        "          new_RFDc = create_rfdc(thrs, cc)\n",
        "          if new_RFDc in RFDcs:\n",
        "            RFDcs.remove(new_RFDc)\n",
        "          RFDcs.append(new_RFDc)\n",
        "\n",
        "    return RFDcs\n",
        "\n",
        "\n",
        "###################################Test#########################################\n",
        "\n",
        "# load difference relation from delta(r) from the previous run\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/btestDelta.pickle', 'rb') as f:\n",
        "  testDelta = pickle.load(f)\n",
        "\n",
        "testDelta_reduced = testDelta[[\"age\", \"workclass\", \"fnlwgt\", \"education\", \"marital-status\", \"occupation\",\n",
        "                \"relationship\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\",\n",
        "                \"native-country\", \"income\"]]\n",
        "#print(len(testDelta))\n",
        "#testDelta_reduced.head(100)\n",
        "# create T_beta\n",
        "#t_zero = create_T_relations(testDelta_reduced, \"income\")\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/btzero.pickle', 'rb') as f:\n",
        "    t_zero = pickle.load(f)\n",
        "\n",
        "#print(t_zero[1])\n",
        "#print(len(t_zero[1]))\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/vectorSample.pickle', 'rb') as f:\n",
        "    sample_vectors = pickle.load(f)\n",
        "\n",
        "\n",
        "# get all viable column combinations for every vector in t_zero\n",
        "def all_lhs_tbeta(T_beta):\n",
        "  lhs_candidates_list = []\n",
        "\n",
        "  for w in T_beta:\n",
        "    lhs_candidates = create_lhs_candidates(T_beta, w)\n",
        "    lhs_candidates_list.append(lhs_candidates)\n",
        "  return lhs_candidates_list\n",
        "\n",
        "#adult_lhs_candidates = all_lhs_tbeta(sample_vectors)\n",
        "\n",
        "#with open('/content/drive/MyDrive/pickledData/btest_lhs_candidates.pickle', 'wb') as f:\n",
        " #   pickle.dump(adult_lhs_candidates, f)\n",
        "with open('/content/drive/MyDrive/pickledData/btest_lhs_candidates.pickle', 'rb') as f:\n",
        "  adult_lhs_candidates = pickle.load(f)\n",
        "\n",
        "#print(adult_lhs_candidates)\n",
        "#print(len(adult_lhs_candidates))\n",
        "#print(\"LHS candidates\",adult_lhs_candidates)\n",
        "\n",
        "# save all tuples of length 4 into a set\n",
        "newRFDCandidates = set()\n",
        "def getrfds(lhs):\n",
        "  for set in adult_lhs_candidates:\n",
        "    for tuple in set:\n",
        "       if len(tuple) ==4:\n",
        "        newRFDCandidates.add(tuple)\n",
        "  return newRFDCandidates\n",
        "\n",
        "#lhs_len_two = getrfds(adult_lhs_candidates)\n",
        "\n",
        "# determine thresholds of viable candidates in lhs_len_four\n",
        "def all_thresholds(TB_relation, LHS):\n",
        "  all_RFDcs = []\n",
        "\n",
        "  for w in TB_relation:\n",
        "    RFDcs_for_w = determine_thresholds(TB_relation, w, LHS)\n",
        "    all_RFDcs.append(RFDcs_for_w)\n",
        "\n",
        "  return all_RFDcs\n",
        "\n",
        "#all_RFDcs_lentwo = all_thresholds(sample_vectors,lhs_len_two)\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/filteredRfdsLenTwo.pickle', 'rb') as f:\n",
        "   filtered_rfds_len_two = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/adultSetRfdsLenthree.pickle', 'rb') as f:\n",
        "  adult_set_rfds_len_three = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/adultSetRfdsLenfour.pickle', 'rb') as f:\n",
        "  adult_set_rfds_len_four = pickle.load(f)\n",
        "\n",
        "#print(\"RFDS\",all_RFDcs_lentwo)\n",
        "#print(len(all_RFDcs))\n",
        "#print(all_RFDcs[1])\n",
        "#print(len(all_RFDcs[1]))\n",
        "\n",
        "def filter_rfds(list_of_rfds, index_to_remove):\n",
        "  for rfd in list_of_rfds:\n",
        "    if index_to_remove in rfd[\"cc\"]:\n",
        "      list_of_rfds.remove(rfd)\n",
        "  return list_of_rfds\n",
        "\n",
        "sample_rfd_len_four = random.sample(adult_set_rfds_len_four, 100)\n",
        "print(sample_rfd_len_four)\n",
        "sample_rfd_len_three = random.sample(adult_set_rfds_len_three, 200)\n",
        "print(sample_rfd_len_three)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biBwjzuJWGs3"
      },
      "source": [
        "#RFD join algorithm\n",
        "\n",
        "The following code handles the anonymization using RFDs as proposed in the paper \"A decision-support framework for data anonymization with\n",
        "application to machine learning processes\", by L.Caruccio et. al.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "@article{CARUCCIO20221,\n",
        "title = {A decision-support framework for data anonymization with application to machine learning processes},\n",
        "journal = {Information Sciences},\n",
        "volume = {613},\n",
        "pages = {1-32},\n",
        "year = {2022},\n",
        "issn = {0020-0255},\n",
        "doi = {https://doi.org/10.1016/j.ins.2022.09.004},\n",
        "url = {https://www.sciencedirect.com/science/article/pii/S0020025522010490},\n",
        "author = {Loredana Caruccio and Domenico Desiato and Giuseppe Polese and Genoveffa Tortora and Nicola Zannone},\n",
        "keywords = {Privacy preserving machine learning, k-anonymity, Relaxed functional dependencies, Generalization strategies}\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucz8F6tHddmF",
        "outputId": "8b6bea76-b96d-4e4b-8931-86278ad077e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6294084033329445\n"
          ]
        }
      ],
      "source": [
        "qis = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"marital-status\", \"occupation\",\n",
        "                \"relationship\", \"sex\",\"capital-gain\", \"capital-loss\", \"hours-per-week\",\n",
        "                \"native-country\"]\n",
        "\n",
        "\n",
        "# find all equivalence classes in a dataset and label them\n",
        "def find_equivalence_classes(dataset, groupby_columns):\n",
        "    dataset_with_classes = dataset.copy()\n",
        "    # new column tracks the equivalence classes\n",
        "    dataset_with_classes['EquivalenceClass'] = dataset_with_classes.groupby(groupby_columns).ngroup()\n",
        "\n",
        "    return dataset_with_classes\n",
        "\n",
        "# find equivalence classes with a specified RFD\n",
        "def find_equivalence_classes_by_rfd(dataset, rfd):\n",
        "    dataset_copy = dataset.copy()\n",
        "    cc = rfd.get(\"cc\")\n",
        "    attributes = []\n",
        "\n",
        "    for value in cc:\n",
        "      attribute = qis[value]\n",
        "      attributes.append(attribute)\n",
        "\n",
        "    dataset_with_classes = dataset.copy()\n",
        "\n",
        "    dataset_with_classes['EquivalenceClass'] = dataset_with_classes.groupby(attributes).ngroup()\n",
        "\n",
        "    return dataset_with_classes\n",
        "\n",
        "# filter dataset to get rows belonging to the specified equivalence class ID\n",
        "def get_equivalence_class_by_id(df, eq_class_id):\n",
        "    equivalence_class = df[df['EquivalenceClass'] == eq_class_id]\n",
        "    return equivalence_class\n",
        "\n",
        "# get frequency of each equivalence class\n",
        "def equivalence_class_sizes(dataset_with_classes):\n",
        "    class_sizes = dataset_with_classes.groupby('EquivalenceClass').size()\n",
        "    size_frequencies = class_sizes.value_counts().sort_values(ascending=False).to_dict()\n",
        "\n",
        "    return size_frequencies\n",
        "\n",
        "\n",
        "\n",
        "# compute anonymization level for k-anonymity\n",
        "def compute_k(dataset):\n",
        "  groups = find_equivalence_classes(dataset, qis)\n",
        "  k = 100\n",
        "  for ec_id, group in groups.groupby('EquivalenceClass'):\n",
        "    if len(group) < k:\n",
        "      k = len(group)\n",
        "\n",
        "  return k\n",
        "\n",
        "# generalize numeric attribute to specified level\n",
        "def generalize_by_threshold_numeric(dataset, attribute, genLevel):\n",
        "\n",
        "  hierarchy = check_hierarchy(attribute)\n",
        "\n",
        "  def gen_by_genLevel(hierarchy, x):\n",
        "    gen_interval = ()\n",
        "\n",
        "    for interval in hierarchy[genLevel-1]:\n",
        "      if x <= interval[1] and x >= interval[0]:\n",
        "        gen_interval = interval\n",
        "        break\n",
        "    return gen_interval\n",
        "\n",
        "  dataset[attribute] = dataset[attribute].apply(lambda x: gen_by_genLevel(hierarchy, x))\n",
        "  return dataset\n",
        "\n",
        "# generalize categoric attribute to specified level\n",
        "def generalize_by_threshold_categoric(dataset, attribute, genLevel):\n",
        "  hierarchy = check_hierarchy(attribute)\n",
        "\n",
        "  if genLevel >1:\n",
        "    dataset[attribute] = dataset[attribute].apply(lambda x: list(hierarchy.keys())[0])\n",
        "  else:\n",
        "    for key, value in hierarchy.items():\n",
        "      if isinstance(value, dict):\n",
        "        for inner_key, inner_value in value.items():\n",
        "          dataset[attribute] = dataset[attribute].apply(lambda x: inner_key if x in inner_value else x)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "# generalize dataset with given RFD\n",
        "\n",
        "def generalize_by_rfd(dataset, rfd):\n",
        "  dataset_copy = dataset.copy()\n",
        "  cc = rfd.get(\"cc\")\n",
        "  thrs = rfd.get(\"thresholds\")\n",
        "  attributes = []\n",
        "\n",
        "  for value in cc:\n",
        "    attribute = qis[value]\n",
        "    attributes.append(attribute)\n",
        "\n",
        "  for attribute in attributes:\n",
        "    if dataset_copy[attribute].dtype == \"int64\":\n",
        "      generalize_by_threshold_numeric(dataset_copy, attribute, thrs[attributes.index(attribute)])\n",
        "    else:\n",
        "      generalize_by_threshold_categoric(dataset_copy, attribute, thrs[attributes.index(attribute)])\n",
        "\n",
        "  all_columns = dataset_copy.columns\n",
        "\n",
        "    # Iterate over columns not in 'attributes' and replace with '*'\n",
        "  for column in all_columns:\n",
        "    if column not in attributes and column != \"income\":\n",
        "      dataset_copy[column] = '*'\n",
        "\n",
        "  dataset_copy['EquivalenceClass'] = dataset_copy.groupby(attributes).ngroup()\n",
        "\n",
        "  return dataset_copy\n",
        "\n",
        "\n",
        "\n",
        "# check, if two attributes are generalized to the same level in two seperate RFDs\n",
        "def has_equal_level(x,y):\n",
        "  cc_x = x.get(\"cc\")\n",
        "  cc_y = y.get(\"cc\")\n",
        "\n",
        "  thrs_x = x.get(\"thresholds\")\n",
        "  thrs_y = y.get(\"thresholds\")\n",
        "\n",
        "  c = set(cc_x).intersection(cc_y)\n",
        "\n",
        "  equal_level = True\n",
        "\n",
        "  for value in c:\n",
        "    if value in cc_x and value in cc_y:\n",
        "      #if thrs_x[cc_x.index(value)] != thrs_y[cc_y.index(value)]:\n",
        "      index_x = cc_x.index(value)\n",
        "      index_y = cc_y.index(value)\n",
        "      if index_x < len(thrs_x) and index_y < len(thrs_y) and thrs_x[index_x] != thrs_y[index_y]:\n",
        "        equal_level = False\n",
        "        break\n",
        "    else:\n",
        "      equal_level = False\n",
        "      break\n",
        "  return equal_level\n",
        "\n",
        "\n",
        "\n",
        "# calculate the NCP of a numeric value\n",
        "def ncp_numeric(dataset, genDataset, attribute):\n",
        "  interval = genDataset[attribute].iloc[0]\n",
        "  if interval == \"*\":\n",
        "    return 1\n",
        "  else:\n",
        "    if isinstance(interval, str) and interval.startswith('(') and interval.endswith(')'):\n",
        "\n",
        "      interval = eval(interval)\n",
        "\n",
        "    lower, upper = interval[0], interval[1] if isinstance(interval, tuple) else (interval, interval)\n",
        "    minVal = dataset[attribute].min()\n",
        "    maxVal = dataset[attribute].max()\n",
        "\n",
        "  return (upper - lower) / (maxVal - minVal)\n",
        "\n",
        "# compute the NCP of a categorical value\n",
        "def ncp_categorical(dataset, genDataset, attribute):\n",
        "  att_hierarchy = check_hierarchy(attribute)\n",
        "\n",
        "  ncp = 0\n",
        "  gen_value = genDataset[attribute].iloc[0]\n",
        "\n",
        "  if gen_value == \"*\":\n",
        "    return 1\n",
        "  else:\n",
        "    for key, value in att_hierarchy.items():\n",
        "      if gen_value == key:\n",
        "        ncp = 1\n",
        "      elif isinstance(value, dict) and gen_value in value.keys():\n",
        "        ncp = len(value.keys()) / len(dataset[attribute].unique())\n",
        "        break\n",
        "  return ncp\n",
        "\n",
        "# check datatype of attribute\n",
        "def kind_of_attribute(attribute):\n",
        "  if attribute in [\"age\", \"fnlwgt\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]:\n",
        "    return \"numeric\"\n",
        "  else:\n",
        "    return \"categorical\"\n",
        "\n",
        "\n",
        "# compute the NCP of an equivalence class\n",
        "def compute_ncp(dataset, genDataset, equivalenceClass):\n",
        "  all_columns = dataset.columns.tolist()\n",
        "  attributes = [col for col in all_columns if col != 'income']\n",
        "  ncp = 0\n",
        "  m = len(attributes)\n",
        "  for attribute in attributes:\n",
        "    if kind_of_attribute(attribute) == \"numeric\":\n",
        "      ncp += ncp_numeric(dataset, equivalenceClass, attribute)\n",
        "    else:\n",
        "      ncp += ncp_categorical(dataset, equivalenceClass, attribute)\n",
        "  return ncp / m\n",
        "\n",
        "# compute the NCP of the whole dataset\n",
        "def compute_gncp(dataset,genDataset):\n",
        "  gncp = 0\n",
        "  for eq_class_id, group in genDataset.groupby('EquivalenceClass'):\n",
        "    gncp += (len(group)/ len(dataset)) * compute_ncp(dataset,genDataset, group)\n",
        "  return gncp\n",
        "\n",
        "# merge two RFDs\n",
        "def rfd_merge(rfd1, rfd2):\n",
        "    combined_cc = tuple(set(rfd1['cc'] + rfd2['cc']))\n",
        "    threshold_map = {}\n",
        "    for i, val in enumerate(rfd1['cc']):\n",
        "        if i < len(rfd1['thresholds']):\n",
        "            threshold_map[val] = rfd1['thresholds'][i]\n",
        "    for i, val in enumerate(rfd2['cc']):\n",
        "        if i < len(rfd2['thresholds']) and val not in threshold_map:\n",
        "            threshold_map[val] = rfd2['thresholds'][i]\n",
        "\n",
        "    if all(val in threshold_map for val in combined_cc):\n",
        "        merged_thresholds = [threshold_map[val] for val in combined_cc]\n",
        "        merged_set = {'thresholds': merged_thresholds, 'cc': combined_cc}\n",
        "        return merged_set\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# merge two RFDs with different sets of column combinations\n",
        "def check_compatibility(rfdsOne, rfdsTwo):\n",
        "  compatible_rfds = []\n",
        "  for i, xi in enumerate(rfdsOne):\n",
        "    for j, yi in enumerate(rfdsTwo):\n",
        "      if i >= j:\n",
        "        continue\n",
        "      X = set(xi.get(\"cc\"))\n",
        "      Y = set(yi.get(\"cc\"))\n",
        "      c = X.intersection(Y)\n",
        "      # Check compatibility of xi and yi\n",
        "      if X.isdisjoint(Y): #or has_equal_level(xi,yi):\n",
        "        ci = rfd_merge(xi,yi)\n",
        "        if ci is not None:\n",
        "          compatible_rfds.append(ci)\n",
        "  return compatible_rfds\n",
        "\n",
        "#compatible_rfds_len_eight = check_compatibility(sample_rfd_len_four)\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/rfdsLenEight.pickle', 'rb') as f:\n",
        "  compatible_rfds_len_eight = pickle.load(f)\n",
        "\n",
        "rfds_len_eight_and_three = []\n",
        "\n",
        "#for rfd in sample_rfd_len_three:\n",
        "  #rfds_len_eight_and_three.append(rfd)\n",
        "\n",
        "#for rfd in compatible_rfds_len_eight:\n",
        "#  rfds_len_eight_and_three.append(rfd)\n",
        "\n",
        "#print(len(rfds_len_eight_and_three))\n",
        "\n",
        "#compatible_rfds_len_eight_and_three = check_compatibility(rfds_len_eight_and_three)\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/rfdsLenEightandThree.pickle', 'rb') as f:\n",
        "  compatible_rfds_len_eight_and_three = pickle.load(f)\n",
        "\n",
        "#print(len(compatible_rfds_len_eight_and_three))\n",
        "#for x in range(50):\n",
        "# print(compatible_rfds_len_eight_and_three[0])\n",
        "\n",
        "\n",
        "def get_unique_rfds(rfds):\n",
        "    unique_rfds = []\n",
        "    seen_rfds = set()\n",
        "\n",
        "    def rfd_to_tuple(rfd):\n",
        "        \"\"\"Converts an RFD dictionary to a tuple for hashable representation.\"\"\"\n",
        "        return (tuple(rfd.get('cc', ())), tuple(rfd.get('thresholds', ())))\n",
        "\n",
        "    for rfd in rfds:\n",
        "        rfd_tuple = rfd_to_tuple(rfd)\n",
        "        if rfd_tuple not in seen_rfds:\n",
        "            seen_rfds.add(rfd_tuple)\n",
        "            unique_rfds.append(rfd)\n",
        "            #if len(unique_rfds) == len(rfds):\n",
        "             #   break  # Stop after collecting 100 unique RFDs\n",
        "    return unique_rfds\n",
        "\n",
        "# Get 200 unique RFDs\n",
        "#unique_rfds_200 = get_200_unique_rfds(compatible_rfds_len_eight_and_three)\n",
        "#print(unique_rfds_200)\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/200uniqueRfds.pickle', 'rb') as f:\n",
        "  unique_rfds_200 = pickle.load(f)\n",
        "\n",
        "#unique_rfds_200_with_len_four = check_compatibility(unique_rfds_200, sample_rfd_len_four)\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/200uniqueRfdsandLenFour.pickle', 'rb') as f:\n",
        "  unique_rfds_200_with_len_four = pickle.load(f)\n",
        "\n",
        "#final_rfds = get_unique_rfds(unique_rfds_200_with_len_four)\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/finalRFds.pickle', 'rb') as f:\n",
        "  final_rfds = pickle.load(f)\n",
        "\n",
        "rfds_with_info = []\n",
        "#for rfd in final_rfds:\n",
        " # r = ()\n",
        " # result = generalize_by_rfd(adult, rfd)\n",
        " # result_with_eq_class = find_equivalence_classes_by_rfd(result, rfd)\n",
        " # k_level =  equivalence_class_sizes(result_with_eq_class)\n",
        " # m = compute_gncp(adult, result_with_eq_class)\n",
        " # r = (rfd, m, k_level)\n",
        " # rfds_with_info.append(r)\n",
        "\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/Rfdswithinfo.pickle', 'rb') as f:\n",
        "  rfds_with_info = pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# find matching equivalence classes for a list of equivalence classes\n",
        "def find_matching_equivalence_classes(dataset, equivalence_class_numbers, k_anonymity_value):\n",
        "    matching_columns =[\"age\", \"workclass\", \"fnlwgt\", \"hours-per-week\"]\n",
        "    matching_classes = {}\n",
        "    minimal_matching_columns = [\"age\", \"workclass\"]  # Define the minimal columns here\n",
        "\n",
        "    for eq_class_num in equivalence_class_numbers:\n",
        "        matching_classes[eq_class_num] = []\n",
        "        current_eq_class_size = len(dataset[dataset['EquivalenceClass'] == eq_class_num])\n",
        "        eq_class_data = dataset[dataset['EquivalenceClass'] == eq_class_num][matching_columns].iloc[0]\n",
        "\n",
        "        # 1. Prioritize matches within equivalence_class_numbers using matching_columns\n",
        "        for other_eq_class_num in equivalence_class_numbers:\n",
        "            if other_eq_class_num != eq_class_num:\n",
        "                other_eq_class_data = dataset[dataset['EquivalenceClass'] == other_eq_class_num][matching_columns].iloc[0]\n",
        "                other_eq_class_size = len(dataset[dataset['EquivalenceClass'] == other_eq_class_num])\n",
        "\n",
        "                if eq_class_data.equals(other_eq_class_data) and other_eq_class_size >= (k_anonymity_value - current_eq_class_size):\n",
        "                    matching_classes[eq_class_num].append(other_eq_class_num)\n",
        "                    if len(matching_classes[eq_class_num]) >= 2:\n",
        "                        break\n",
        "\n",
        "        # 2. Search entire dataset using minimal_matching_columns if no matches found within\n",
        "        if not matching_classes[eq_class_num]:\n",
        "            eq_class_data_minimal = dataset[dataset['EquivalenceClass'] == eq_class_num][minimal_matching_columns].iloc[0]\n",
        "            for other_eq_class_num in dataset['EquivalenceClass'].unique():\n",
        "                if other_eq_class_num not in equivalence_class_numbers:  # Exclude those already checked\n",
        "                    other_eq_class_data_minimal = dataset[dataset['EquivalenceClass'] == other_eq_class_num][minimal_matching_columns].iloc[0]\n",
        "                    other_eq_class_size = len(dataset[dataset['EquivalenceClass'] == other_eq_class_num])\n",
        "\n",
        "                    if eq_class_data_minimal.equals(other_eq_class_data_minimal) and other_eq_class_size >= (k_anonymity_value - current_eq_class_size):\n",
        "                        matching_classes[eq_class_num].append(other_eq_class_num)\n",
        "                        if len(matching_classes[eq_class_num]) >= 2:\n",
        "                            break\n",
        "\n",
        "    return matching_classes\n",
        "\n",
        "\n",
        "def find_matching_l_diverse_classes(dataset, target_eq_class_id, exclude_eq_class_ids, l_diversity_value):\n",
        "\n",
        "    matching_columns = [\"age\", \"fnlwgt\"]  # Attributes to match on\n",
        "    sensitive_attribute = \"income\"  # Sensitive attribute for l-diversity check\n",
        "    matching_classes = []\n",
        "\n",
        "    # Get data for the target equivalence class\n",
        "    target_eq_class_data = dataset[dataset['EquivalenceClass'] == target_eq_class_id][matching_columns].iloc[0]\n",
        "    target_eq_class_size = len(dataset[dataset['EquivalenceClass'] == target_eq_class_id])\n",
        "\n",
        "    # Get all unique equivalence class IDs except those in exclude_eq_class_ids\n",
        "    all_eq_class_ids = dataset['EquivalenceClass'].unique()\n",
        "    eq_class_ids_to_search = [eq_id for eq_id in all_eq_class_ids if eq_id not in exclude_eq_class_ids and eq_id != target_eq_class_id]\n",
        "\n",
        "    # Search for matching l-diverse classes\n",
        "    for eq_class_id in eq_class_ids_to_search:\n",
        "        # Check for matching attributes\n",
        "        eq_class_data = dataset[dataset['EquivalenceClass'] == eq_class_id][matching_columns].iloc[0]\n",
        "        eq_class_size = len(dataset[dataset['EquivalenceClass'] == eq_class_id])\n",
        "\n",
        "        # Check if class size is at least k - len(current equality class) and if it is l-diverse\n",
        "        if target_eq_class_data.equals(eq_class_data) and is_l_diverse(dataset[dataset['EquivalenceClass'] == eq_class_id], sensitive_attribute, l_diversity_value):\n",
        "            matching_classes.append(eq_class_id)\n",
        "\n",
        "    return matching_classes\n",
        "\n",
        "def find_matching_k_classes(dataset, target_eq_class_id, exclude_eq_class_ids, k_anonymity_value):\n",
        "\n",
        "    matching_columns = [\"age\", \"fnlwgt\"]  # Attributes to match on\n",
        "    sensitive_attribute = \"income\"  # Sensitive attribute for l-diversity check\n",
        "    matching_classes = []\n",
        "\n",
        "    # Get data for the target equivalence class\n",
        "    target_eq_class_data = dataset[dataset['EquivalenceClass'] == target_eq_class_id][matching_columns].iloc[0]\n",
        "    target_eq_class_size = len(dataset[dataset['EquivalenceClass'] == target_eq_class_id])\n",
        "\n",
        "    # Get all unique equivalence class IDs except those in exclude_eq_class_ids\n",
        "    all_eq_class_ids = dataset['EquivalenceClass'].unique()\n",
        "    eq_class_ids_to_search = [eq_id for eq_id in all_eq_class_ids if eq_id not in exclude_eq_class_ids and eq_id != target_eq_class_id]\n",
        "\n",
        "    # Search for matching l-diverse classes\n",
        "    for eq_class_id in eq_class_ids_to_search:\n",
        "        # Check for matching attributes\n",
        "        eq_class_data = dataset[dataset['EquivalenceClass'] == eq_class_id][matching_columns].iloc[0]\n",
        "        eq_class_size = len(dataset[dataset['EquivalenceClass'] == eq_class_id])\n",
        "\n",
        "        # Check if class size is at least k - len(current equality class) and if it is l-diverse\n",
        "        if target_eq_class_data.equals(eq_class_data) and eq_class_size >= (k_anonymity_value - target_eq_class_size):\n",
        "            matching_classes.append(eq_class_id)\n",
        "\n",
        "    return matching_classes\n",
        "\n",
        "\n",
        "def find_not_k_anonymous_eq_class(dataset, k):\n",
        "  not_k_anonymous_classes = []\n",
        "  for eq_class_id, group in dataset.groupby('EquivalenceClass'):\n",
        "    if len(group) < k:\n",
        "      #print(f\"\\nEquivalence Class {eq_class_id}:\\n{group}\")\n",
        "      #print(len(group))\n",
        "      not_k_anonymous_classes.append(eq_class_id)\n",
        "  return not_k_anonymous_classes\n",
        "\n",
        "\n",
        "\n",
        "def anonymize_equivalence_classes(dataset, equivalence_class_ids):\n",
        "    anonymized_dataset = dataset.copy()\n",
        "\n",
        "    columns_to_anonymize = [col for col in anonymized_dataset.columns if col != 'EquivalenceClass']\n",
        "    for eq_class_id in equivalence_class_ids:\n",
        "        # Filter the dataset for the current equivalence class\n",
        "        equivalence_class_data = anonymized_dataset[anonymized_dataset['EquivalenceClass'] == eq_class_id]\n",
        "\n",
        "        # Replace values in the specified columns with '*'\n",
        "        for column in columns_to_anonymize:\n",
        "            anonymized_dataset.loc[equivalence_class_data.index, column] = '*'\n",
        "\n",
        "    return anonymized_dataset\n",
        "\n",
        "# generalize intervals for the merge operation of two equivalence classes\n",
        "def generalize_intervals(hierarchy, intervalOne, intervalTwo, genLevel):\n",
        "  gen_interval = ()\n",
        "\n",
        "  for interval in hierarchy[genLevel-1]:\n",
        "    if intervalTwo[1] <= interval[1] and intervalOne[0] >= interval[0]:\n",
        "      gen_interval = interval\n",
        "      break\n",
        "    elif intervalOne[1] <= interval[1] and intervalTwo[0] >= interval[0]:\n",
        "      gen_interval = interval\n",
        "      break\n",
        "  return gen_interval\n",
        "\n",
        "\n",
        "def merge_equivalence_classes(dataset, not_k_anon_class_id, k_anon_class_id):\n",
        "\n",
        "    # Get the rows for the two equivalence classes\n",
        "    eq_class1_rows = dataset[dataset['EquivalenceClass'] == not_k_anon_class_id]\n",
        "    eq_class2_rows = dataset[dataset['EquivalenceClass'] == k_anon_class_id]\n",
        "\n",
        "    # Get a sample row from each equivalence class for modification\n",
        "    sample_row1 = eq_class1_rows.iloc[0].copy()\n",
        "    sample_row2 = eq_class2_rows.iloc[0].copy()\n",
        "\n",
        "    # Iterate over columns, generalize values if not in exclude_columns\n",
        "    for column in dataset.columns:\n",
        "        if column not in [\"income\", 'EquivalenceClass']:  # Exclude EquivalenceClass column\n",
        "            value1 = sample_row1[column]\n",
        "            value2 = sample_row2[column]\n",
        "\n",
        "            if value1 == value2:\n",
        "              continue\n",
        "\n",
        "            elif kind_of_attribute(column) == \"numeric\" and value1 != \"*\":\n",
        "              left = min(value1[0], value2[0])\n",
        "              right = max(value1[0], value2[0])\n",
        "\n",
        "              sample_row1[column] = (left,right)\n",
        "              sample_row2[column] = (left,right)\n",
        "            elif kind_of_attribute(column) == \"categorical\":\n",
        "\n",
        "              sample_row1[column] = \"*\"\n",
        "              sample_row2[column] = \"*\"\n",
        "\n",
        "    for column in dataset.columns:\n",
        "        if column not in [\"income\", 'EquivalenceClass']:\n",
        "            dataset.loc[dataset['EquivalenceClass'] == not_k_anon_class_id, column] = str(sample_row1[column])\n",
        "            dataset.loc[dataset['EquivalenceClass'] == k_anon_class_id, column] = str(sample_row2[column])\n",
        "    # Merge equivalence classes by changing eq_class_id2 to eq_class_id1\n",
        "    dataset.loc[dataset['EquivalenceClass'] == not_k_anon_class_id, 'EquivalenceClass'] = k_anon_class_id\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "# make sure the dataset is l-diverse\n",
        "def achieve_l_diversity(dataset, sensitive_attribute, l_diversity_value):\n",
        "\n",
        "\n",
        "    previous_num_not_l_diverse = float('inf')\n",
        "\n",
        "    while True:\n",
        "        not_l_diverse_classes = []\n",
        "        for eq_class_id, group in dataset.groupby('EquivalenceClass'):\n",
        "            if not is_l_diverse(group, sensitive_attribute, l_diversity_value):\n",
        "                not_l_diverse_classes.append(eq_class_id)\n",
        "\n",
        "        if not not_l_diverse_classes:  # If all classes are l-diverse, exit\n",
        "            break\n",
        "\n",
        "        # Check if the number of non-l-diverse classes has not decreased\n",
        "        if len(not_l_diverse_classes) == previous_num_not_l_diverse:\n",
        "            print(\"Warning: Cannot achieve l-diversity with current matching criteria.\")\n",
        "            break\n",
        "\n",
        "        previous_num_not_l_diverse = len(not_l_diverse_classes)\n",
        "\n",
        "        #matching_classes_dict = find_matching_equivalence_classes(dataset, not_l_diverse_classes, k_anonymity_value)\n",
        "\n",
        "        for eq_class_id in not_l_diverse_classes:\n",
        "          found_matches = find_matching_l_diverse_classes(dataset, eq_class_id, not_l_diverse_classes, l_diversity_value)\n",
        "            #matches = matching_classes_dict.get(eq_class_id, [])\n",
        "\n",
        "            # Filter matches to exclude classes in not_l_diverse_classes\n",
        "            #valid_matches = [match for match in matches if match not in not_l_diverse_classes]\n",
        "\n",
        "          if found_matches !=[]:\n",
        "            merge_with = min(found_matches)  # Choose the smallest valid matching class ID\n",
        "            dataset = merge_equivalence_classes(dataset, eq_class_id, merge_with)\n",
        "          elif found_matches == []:\n",
        "            continue\n",
        "    return dataset\n",
        "\n",
        "#l_div_5_anon_adult = achieve_l_diversity(sevenfive_anonymized_dataset, \"income\",2)\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/RFDl_div_5_anon_adult.pickle', 'rb') as f:\n",
        "  l_div_100_anon_adult = pickle.load(f)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXCM9VG95ZVl"
      },
      "outputs": [],
      "source": [
        "# Apply above functions to the obesity dataset\n",
        "\n",
        "\n",
        "\n",
        "# create hierarchies\n",
        "CAEC_hierarchy = {\n",
        "    \"*\": {\n",
        "        \"Does Not Eat Between Meals\": [\"no\"],\n",
        "        \"Eats Between Meals\": [\"Sometimes\", \"Frequently\", \"Always\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "CALC_hierarchy = {\n",
        "    \"*\": {\n",
        "        \"does not drink alcohol\": [\"no\"],\n",
        "        \"drinks alcohol\": [\"Sometimes\", \"Frequently\", \"Always\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "MTRANS_hierarchy = {\n",
        "    \"*\": {\n",
        "        \"Motorized Transportation\": [\"Public_Transportation\", \"Automobile\", \"Motorbike\"],\n",
        "        \"Non-Motorized Transportation\": [\"Walking\", \"Bike\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "nobeyesdad_hierarchy ={\"NObeyesdad\": ['Normal_Weight' 'Overweight_Level_I' 'Overweight_Level_II'\n",
        " 'Obesity_Type_I' 'Insufficient_Weight' 'Obesity_Type_II'\n",
        " 'Obesity_Type_III']}\n",
        "\n",
        "# fam_history_with_overweight, FAVC, SMOKE, SCC\n",
        "binary_hierarchy = {\"*\": [\"yes\", \"no\"]}\n",
        "\n",
        "gender_hierarchy = {\"*\": [\"Male\", \"Female\"]}\n",
        "\n",
        "\n",
        "\n",
        "def generalize_numeric_obesity(dataset, qi):\n",
        "  min_val, max_val = dataset[qi].min(), dataset[qi].max()\n",
        "  if min_val == max_val:\n",
        "    dataset[qi] = \"*\"\n",
        "  elif qi == \"Age\":\n",
        "    min_val = min_val - min_val % 5\n",
        "    max_val = max_val + 5 - (max_val % 5)\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  elif qi == \"Height\":\n",
        "    min_val = math.floor(min_val)\n",
        "    max_val = math.ceil(max_val)\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  elif qi == \"Weight\":\n",
        "    min_val = min_val - min_val % 5\n",
        "    max_val = max_val + 5 - (max_val % 5)\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  elif qi == \"FCVC\":\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  elif qi == \"NCP\":\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  elif qi == \"CH2O\":\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  elif qi == \"FAF\":\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  elif qi == \"TUE\":\n",
        "    dataset[qi] = f\"[{min_val} - {max_val}]\"\n",
        "  return dataset\n",
        "\n",
        "FCVC_hierarchy = [[(1.5,1), (1.5,2), (2,2,5), (2.5,3)],[(1,2),(2,3)], [(1,3)]]\n",
        "NCP_hierarchy = [[(1,1.5), (1.5,2), (2,2.5), (2.5,3), (3.5,4)],[(1,2),(2,3),(3,4)],[(1,2),(2,4)], [(1,4)]]\n",
        "height_hierarchy = [[(1,1.5),(1.5,2)], [(1,2)]]\n",
        "tue_hierarchy = [[(0.0, 0.5), (0.5, 1.0), (1.0, 1.5), (1.5, 2.0)], [(0.0, 1.0), (1.0, 2.0)], [(0.0, 2.0)]]\n",
        "CH2O_hierarchy = [[(1.5,1), (1.5,2), (2.5,3)],[(1,2),(2,3)], [(1,3)]]\n",
        "FAF_hierarchy = [[(0,0.5),(0.5,1), (1,1.5), (1.5,2),(2,2.5), (2.5,3)], [(0,1), (1,2), (2,3)], [(0,2), (2,3)], [(0,3)]]\n",
        "weight_values = obesityDataset[\"Weight\"].unique()\n",
        "weight_hierarchy = create_gen_hierarchy(weight_values, 5)\n",
        "#print(weight_hierarchy)\n",
        "age_obesity_values = obesityDataset[\"Age\"].unique()\n",
        "age_obesity_hierarchy = create_gen_hierarchy(age_obesity_values, 5)\n",
        "#print(age_obesity_hierarchy)\n",
        "weight_values = obesityDataset[\"Weight\"].unique()\n",
        "weight_hierarchy = create_gen_hierarchy(weight_values, 5)\n",
        "#print(weight_hierarchy)\n",
        "#print(age_obesity_hierarchy)\n",
        "nobeyesdad_obesity_values = obesityDataset[\"NObeyesdad\"].unique()\n",
        "\n",
        "\n",
        "\n",
        "def check_obesity_hierarchy(attribute):\n",
        "  if attribute == 'Gender':\n",
        "    return gender_hierarchy\n",
        "  elif attribute == 'Age':\n",
        "    return age_obesity_hierarchy\n",
        "  elif attribute == 'Height':\n",
        "    return height_hierarchy\n",
        "  elif attribute == 'Weight':\n",
        "    return weight_hierarchy\n",
        "  elif attribute == 'FCVC':\n",
        "    return FCVC_hierarchy\n",
        "  elif attribute == 'NCP':\n",
        "    return NCP_hierarchy\n",
        "  elif attribute == \"CAEC\":\n",
        "    return CAEC_hierarchy\n",
        "  elif attribute == \"CH2O\":\n",
        "    return CH2O_hierarchy\n",
        "  elif attribute == \"FAF\":\n",
        "    return FAF_hierarchy\n",
        "  elif attribute == \"TUE\":\n",
        "    return tue_hierarchy\n",
        "  elif attribute == \"CALC\":\n",
        "    return CALC_hierarchy\n",
        "  elif attribute == \"MTRANS\":\n",
        "    return MTRANS_hierarchy\n",
        "  elif attribute == \"NObeyesdad\":\n",
        "    return nobeyesdad_hierarchy\n",
        "  else:\n",
        "    return binary_hierarchy\n",
        "\n",
        "\n",
        "def is_k_anonymous_categorical(partition, categorical_qis):\n",
        "\n",
        "    # Select only the categorical quasi-identifier columns\n",
        "    qi_data = partition[categorical_qis]\n",
        "\n",
        "    # Count the occurrences of unique combinations of categorical QI values\n",
        "    counts = qi_data.groupby(categorical_qis).size()\n",
        "\n",
        "    # Check if all counts are greater than or equal to k\n",
        "    # and explicitly return True or False\n",
        "    k = len(partition)\n",
        "    if (counts >= k).all():\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def refine_generalization(dataset, qi):\n",
        "# Work on a copy to avoid modifying the original\n",
        "\n",
        "    #for qi in qis:\n",
        "      #  if kind_of_attribute(qi) == \"categorical\":  # Numeric or categorical that has been generalized\n",
        "\n",
        "  hierarchy = check_obesity_hierarchy(qi)\n",
        "  if isinstance(hierarchy, list):\n",
        "    print(f\"Attribute: {qi}, Hierarchy Type: {type(hierarchy)}\")\n",
        "  if hierarchy is not None and isinstance(hierarchy, dict):\n",
        "    dataset[qi] = dataset[qi].apply(lambda x: gen_to_higher_level(x, hierarchy))\n",
        "\n",
        "  return dataset\n",
        "\n",
        "# make partition k-anonymous\n",
        "def ensure_k_anonymity_categorical(partition):\n",
        "  categorical_qis = ['Gender', 'family_history_with_overweight', 'FAVC', 'SMOKE','SCC', 'CAEC', 'CALC', 'MTRANS']\n",
        "  #[\"workclass\", \"education\", \"marital-status\", \"relationship\", \"occupation\", \"native-country\"]\n",
        "\n",
        "  generalized_partition = partition.copy()\n",
        "  while not is_k_anonymous_categorical(generalized_partition, categorical_qis):\n",
        "        # Find categorical columns with multiple values\n",
        "        columns_to_generalize = [\n",
        "            col for col in categorical_qis\n",
        "            if generalized_partition[col].nunique() > 1  # More than one unique value\n",
        "        ]\n",
        "\n",
        "        if not columns_to_generalize:\n",
        "            # If no columns can be generalized further, break the loop\n",
        "            break\n",
        "\n",
        "        # Generalize the first column in the list (minimal generalization)\n",
        "        column_to_generalize = columns_to_generalize[0]\n",
        "        print(column_to_generalize)\n",
        "        hierarchy = check_obesity_hierarchy(column_to_generalize)\n",
        "\n",
        "        generalized_partition[column_to_generalize] = generalized_partition[column_to_generalize].apply(lambda x: gen_to_higher_level(x, hierarchy))\n",
        "\n",
        "\n",
        "  return generalized_partition\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generalize(dataset,qis):\n",
        "  for qi in qis:\n",
        "    if kind_of_attribute(qi) == \"numeric\":\n",
        "      dataset = generalize_numeric_obesity(dataset, qi)\n",
        "    else:\n",
        "      dataset = refine_generalization(dataset, qi)\n",
        "\n",
        "  dataset = ensure_k_anonymity_categorical(dataset)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This recursive function checks for k-anonymity and l-diversity in each recursion\n",
        "step. The algorithm is based on the algorithm of LeFevre et. al., as stated above.\n",
        "\"\"\"\n",
        "def l_mondrian(dataset, k, qis, l, sensitive_attribute):\n",
        "    global recursive_call\n",
        "\n",
        "\n",
        "\n",
        "    # Base cases for recursion\n",
        "    #print(len(dataset))\n",
        "    #print(entropy_partition(dataset, sensitive_attribute))\n",
        "    if len(dataset) < 2 * k:\n",
        "      return generalize(dataset.copy(), qis)  # Use a copy to ensure isolation\n",
        "    if recursive_call == 0:\n",
        "      selected_qi = \"Age\"\n",
        "    else:\n",
        "      selected_qi = widest_qi(dataset, qis)\n",
        "\n",
        "    print(selected_qi)\n",
        "    #selected_qi = widest_qi(dataset, qis)  # choose QI with the most unique values\n",
        "    #print(f\"Selected QI: {selected_qi}\")  # Debug print\n",
        "\n",
        "    # Handle numeric attributes\n",
        "    if dataset[selected_qi].dtype == np.float64:   #\"int64\"\n",
        "        sorted_dataset = dataset.sort_values(by=selected_qi).copy()  # Ensure a copy for safety\n",
        "        middle_index = len(sorted_dataset) // 2\n",
        "        left_side = sorted_dataset.iloc[:middle_index].copy()\n",
        "        right_side = sorted_dataset.iloc[middle_index:].copy()\n",
        "\n",
        "    # Handle categorical attributes\n",
        "    elif dataset[selected_qi].dtype == object:\n",
        "        print(selected_qi)\n",
        "        hierarchy = check_obesity_hierarchy(selected_qi)\n",
        "\n",
        "        dataset[selected_qi] = dataset[selected_qi].apply(lambda x: gen_to_higher_level(x, hierarchy))  # Isolate generalized data\n",
        "        sorted_partition = sorted(dataset[selected_qi].unique())\n",
        "        split_value = find_middle(sorted_partition)\n",
        "        group_left = sorted_partition[:sorted_partition.index(split_value)]\n",
        "        group_right = sorted_partition[sorted_partition.index(split_value):]\n",
        "        left_side = dataset[dataset[selected_qi].isin(group_left)].copy()\n",
        "        right_side = dataset[dataset[selected_qi].isin(group_right)].copy()\n",
        "\n",
        "    # Recursively apply l_mondrian on the partitions\n",
        "    if (len(left_side) >=k and is_l_diverse(left_side, sensitive_attribute, l)) and (len(right_side) >= k and is_l_diverse(right_side, sensitive_attribute, l)):\n",
        "      recursive_call += 1\n",
        "      partition_one = l_mondrian(left_side, k, qis, l, sensitive_attribute)\n",
        "      partition_two = l_mondrian(right_side, k, qis, l, sensitive_attribute)\n",
        "\n",
        "\n",
        "    else:\n",
        "      return generalize(dataset.copy(), qis)\n",
        "\n",
        "\n",
        "    # Concatenate the generalized partitions\n",
        "    return pd.concat([partition_one, partition_two])\n",
        "\n",
        "\n",
        "\n",
        "distance_data_obesity = []\n",
        "\n",
        "# test L_mondrian\n",
        "#qi_columns= [\"Gender\", \"Age\", \"Height\", \"Weight\",\"family_history_with_overweight\",\"FAVC\", \"FCVC\", \"NCP\", \"CAEC\", \"SMOKE\", \"CH2O\",\"SCC\", \"FAF\", \"TUE\", \"CALC\", \"MTRANS\",\"NObeyesdad\"]\n",
        "#nine_mondrian_obesity_50_anon = l_mondrian(obesityDataset, 10,['Age','Height', 'Weight','FCVC','NCP', 'CH2O', 'FAF', 'TUE','MTRANS'],9 ,\"NObeyesdad\")\n",
        "#nine_mondrian_obesity_10_anon_eq = find_equivalence_classes(nine_mondrian_obesity_50_anon, qi_columns)\n",
        "#display(nine_mondrian_obesity_five_anon)\n",
        "\n",
        "\n",
        "#with open('/content/drive/MyDrive/pickledData/nine_mondrian_obesity_10_anon_eq.pickle', 'wb') as f:\n",
        " #   pickle.dump(nine_mondrian_obesity_10_anon_eq, f)\n",
        "#with open('/content/drive/MyDrive/pickledData/nine_mondrian_obesity_75_anon_eq.pickle', 'rb') as f:\n",
        "#    nine_mondrian_obesity_100_anon_eq = pickle.load(f)\n",
        "\n",
        "# create distance matrix with generalization distance as metric\n",
        "def compute_delta_df(dataset, num_rows, qi_columns, sensitive_attribute):\n",
        "  for i in range(num_rows):\n",
        "    for j in range(i + 1, num_rows):\n",
        "        row_distances = {}\n",
        "        for column in dataset.columns:\n",
        "            if column in qi_columns:\n",
        "                if dataset[column].dtype == \"float64\":\n",
        "                    distance = generalization_distance_numeric(dataset.loc[i, column], dataset.loc[j, column], check_obesity_hierarchy(column))\n",
        "                else:\n",
        "                    distance = generalization_distance_categoric(dataset.loc[i, column], dataset.loc[j, column], check_obesity_hierarchy(column))\n",
        "                row_distances[column] = distance\n",
        "            else:\n",
        "                row_distances[column] = None\n",
        "        distance_data_obesity.append(row_distances)\n",
        "  distance_df = pd.DataFrame(distance_data_obesity)\n",
        "  return distance_df.sort_values(by=sensitive_attribute)\n",
        "\n",
        "#testDelta = compute_delta_df(testDM_sample, test_rows, test_columns, \"income\")\n",
        "\n",
        "obesity_qi_columns= [\"Gender\", \"Age\", \"Height\", \"Weight\",\"family_history_with_overweight\",\"FAVC\", \"FCVC\", \"NCP\", \"CAEC\", \"SMOKE\", \"CH2O\",\"SCC\", \"FAF\", \"TUE\", \"CALC\", \"MTRANS\",\"NObeyesdad\"]\n",
        "#obesity_sample = obesityDataset.head(150)\n",
        "\n",
        "#obesity_delta = compute_delta_df(obesity_sample, 150, obesity_qi_columns, \"NObeyesdad\")\n",
        "#len(obesity_delta)\n",
        "#display(obesity_delta)\n",
        "\n",
        "\n",
        "\n",
        "# create T_beta\n",
        "#t_zero_obesity = create_T_relations(obesity_delta, \"NObeyesdad\")\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/btzeroObesity.pickle', 'rb') as f:\n",
        "    t_zero_obesity = pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#obesity_lhs_candidates = all_lhs_tbeta(t_zero_obesity[0])\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/pickledData/obesity_lhs_candidates.pickle', 'rb') as f:\n",
        "  obesity_lhs_candidates = pickle.load(f)\n",
        "\n",
        "\n",
        "newRFDCandidates_obesity = set()\n",
        "#def getrfds(lhs):\n",
        " # for set in obesity_lhs_candidates:\n",
        "  #  for tuple in set:\n",
        "   #    if len(tuple) ==3:\n",
        "    #    newRFDCandidates_obesity.add(tuple)\n",
        " # return newRFDCandidates_obesity\n",
        "\n",
        "#lhs_len_three_obesity = getrfds(obesity_lhs_candidates)\n",
        "\n",
        "#all_rfds_len_three_obesity = all_thresholds(t_zero_obesity[0],lhs_len_three_obesity)\n",
        "\n",
        "#with open('/content/drive/MyDrive/pickledData/obesityLenThreeRFDs.pickle', 'wb') as f:\n",
        " #   pickle.dump(all_rfds_len_three_obesity, f)\n",
        "#with open('/content/drive/MyDrive/pickledData/obesityLenThreeRFDs.pickle', 'rb') as f:\n",
        " # all_rfds_len_three_obesity = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/pickledData/obesityLenFourRFDs.pickle', 'rb') as f:\n",
        "  all_rfds_len_four_obesity = pickle.load(f)\n",
        "\n",
        "print(all_rfds_len_four_obesity)\n",
        "\n",
        "#obesity_comatible_rfds_len_eight = check_compatibility(all_rfds_len_four_obesity, all_rfds_len_four_obesity)\n",
        "#print(obesity_comatible_rfds_len_eight)\n",
        "\n",
        "#with open('/content/drive/MyDrive/pickledData/obesity_comatible_rfds_len_eight.pickle', 'wb') as f:\n",
        " #   pickle.dump(obesity_comatible_rfds_len_eight, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adultresult = pd.read"
      ],
      "metadata": {
        "id": "ZCF1YQx_1pwQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1ETw-3PjpzhWklcO6tpn5saw5xchSVkTg",
      "authorship_tag": "ABX9TyPlGAG9OgHfzfB7TuJ1yXxO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}